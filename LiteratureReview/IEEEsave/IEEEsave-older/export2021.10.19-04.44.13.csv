"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Applicable Micropatches and Where to Find Them: Finding and Applying New Security Hot Fixes to Old Software","M. Malone; Y. Wang; K. Snow; F. Monrose","UNC,Chapel Hill,USA; UNC,Chapel Hill,USA; Zeropoint Dynamics,USA; UNC,Chapel Hill,USA","2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)","24 May 2021","2021","","","394","405","With the complexity and interdependency of modern software sharply rising, the impact of security vulnerabilities and thus the value of broadly available patches has increased drastically. Despite this, it is unclear if the current landscape supports the same level of patch discoverability as that of vulnerabilities - raising questions about whether patches are simply scare or if they are just hard to find (i.e., are there a lot of ""secret patches"" [1]), and, equally important, what kind of patches are they (e.g., are they micropatchable). We seek to assess the current state of patching by analyzing patches for a four-month period of recent Common Vulnerabilities and Exposures (CVEs). At first glance, the state of patching seems abysmal - only one-fourth of CVEs have a labelled patch on the National Vulnerability Database (NVD). However, by searching for indicators on other popular security trackers (e.g., Debian's), we were able to find a lot more ""secret patches"", but the ratio of patched CVEs plateaued around fifty percent.Examining the discovered patches, we noticed that many were version updates, and less than one-tenth had machine accessible source-code micropatches. Using a custom tool that leverages contemporary version control, we were able to test the feasibility of automatically applying these micropatches to older versions of the software and found that approximately two-thirds of the patches can be applied to at least one old version. The failure cases were mostly due to lax practices pertaining to security fixes and general software development (e.g., releasing the fix along with other extraneous features). Reflecting on our investigations, we surmise that between existence, discoverability, and versatility of security patches, existence and discoverability are the bigger problems. As to why this is the case, we find that the answer may lie in the perverse incentive structures of the industry. We conclude with possible remediations and hope that our work at least raises public awareness of the current state of patching and encourages future work to improve the situation.","2159-4848","978-1-7281-6836-4","10.1109/ICST49551.2021.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438561","micropatching;patch discovery;patch testing","Software testing;Industries;Databases;Conferences;Tools;Software;Complexity theory","computational complexity;security of data;software maintenance","applicable micropatches;applying new security hot fixes;old software;security vulnerabilities;broadly available patches;patch discoverability;secret patches;common vulnerabilities;labelled patch;national vulnerability database;popular security trackers;discovered patches;machine accessible source-code micropatches;security fixes;security patches;patched CVE","","","","67","","24 May 2021","","","IEEE","IEEE Conferences"
"X10X: Model Checking a New Programming Language with an ""Old"" Model Checker","M. Gligoric; P. C. Mehlitz; D. Marinov","Univ. of Illinois, Urbana, IL, USA; NASA Ames Res. Center, Moffett Field, CA, USA; Univ. of Illinois, Urbana, IL, USA","2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","17 May 2012","2012","","","11","20","Parallel and distributed computing is becoming a norm with the advent of multi-core, networked, and cloud computing platforms. New programming languages are emerging for these platforms, e.g., the X10 language from IBM. While these languages explicitly support concurrent programming, they cannot eliminate all concurrency related bugs, which are usually hard to find. Finding such bugs is easier using specialized, language-aware model-checking tools. However, such tools are highly complex and developing them from scratch requires large effort. We describe our experience in developing a model-checking tool for a new language, X10, by systematically adapting an existing tool, the JPF model checker for Java. X10 programs can be compiled to Java, but unfortunately checking X10 programs directly with the unmodified JPF and X10 runtime can miss some behaviors and scales very poorly. We present four sets of techniques that can be employed to make checking a new language with an "" old"" model checker practical: (1) modify the model checker, (2) modify the language runtime, (3) extend the language compiler, and (4) develop a new static analysis. We instantiated each technique to enable checking X10 programs with JPF. We evaluated our new techniques on over 100 X10 example programs and found a substantial speedup.","2159-4848","978-0-7695-4670-4","10.1109/ICST.2012.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200092","","Java;Runtime;Arrays;Computer bugs;Histograms;Object oriented modeling;Programming","concurrency control;Java;program compilers;program debugging;program diagnostics;program verification","X10X;programming language;parallel computing;distributed computing;concurrent programming;concurrency related bugs;language-aware model-checking tool;JPF model checker;Java;X10 program;old model checker;language runtime;language compiler;static analysis","","13","","31","","17 May 2012","","","IEEE","IEEE Conferences"
"Canopus: A Domain-Specific Language for Modeling Performance Testing","M. Bernardino; A. F. Zorzo; E. M. Rodrigues","Fac. of Inf., Pontifical Catholic Univ. of Rio Grande do Sul, Porto Alegre, Brazil; Fac. of Inf., Pontifical Catholic Univ. of Rio Grande do Sul, Porto Alegre, Brazil; Fac. of Inf., Pontifical Catholic Univ. of Rio Grande do Sul, Porto Alegre, Brazil","2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)","21 Jul 2016","2016","","","157","167","Despite all the efforts to reduce the cost of the testing phase in software development, it is still one of the most expensive phases. In order to continue to minimize those costs, in this paper, we propose a Domain-Specific Language (DSL), built on top of MetaEdit+ language workbench, to model performance testing for web applications. Our DSL, called Canopus, was developed in the context of a collaboration<sup>1</sup> between our university and a Technology Development Laboratory (TDL) from an Information Technology (IT) company. We present, in this paper, the Canopus metamodels, its domain analysis, a process that integrates Canopus to Model-Based Performance Testing, and applied it to an industrial case study.","","978-1-5090-1827-7","10.1109/ICST.2016.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515468","performance testing;domain-specific language;domain-specific modeling;model-based testing","Testing;DSL;Unified modeling language;Analytical models;Load modeling;Computational modeling;Automation","Internet;program testing;software cost estimation;specification languages","domain-specific language;performance testing modeling;cost reduction;software development testing;cost minimization;DSL;MetaEdit+ language workbench;Web applications;technology development laboratory;TDL;information technology company;IT company;Canopus metamodels;model-based performance testing","","7","","32","","21 Jul 2016","","","IEEE","IEEE Conferences"
"A Framework to Support Research in and Encourage Industrial Adoption of Regression Testing Techniques","J. M. Kauffman; G. M. Kapfhammer",NA; NA,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","17 May 2012","2012","","","907","908","When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.","2159-4848","978-0-7695-4670-4","10.1109/ICST.2012.194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200107","open-source framework;regression testing","Testing;Open source software;Monitoring;Timing;Algorithm design and analysis;Java","fault diagnosis;program testing;public domain software;regression analysis","industrial adoption;regression testing techniques;software developers;regression test suite;old functionality;test cases;reduction techniques;prioritization techniques;open-source framework","","","","8","","17 May 2012","","","IEEE","IEEE Conferences"
"Automated Behavioral Regression Testing","W. Jin; A. Orso; T. Xie","Georgia Inst. of Technol., Atlanta, GA, USA; Georgia Inst. of Technol., Atlanta, GA, USA; North Carolina State Univ., Raleigh, NC, USA","2010 Third International Conference on Software Testing, Verification and Validation","3 Jun 2010","2010","","","137","146","When a program is modified during software evolution, developers typically run the new version of the program against its existing test suite to validate that the changes made on the program did not introduce unintended side effects (i.e., regression faults). This kind of regression testing can be effective in identifying some regression faults, but it is limited by the quality of the existing test suite. Due to the cost of testing, developers build test suites by finding acceptable tradeoffs between cost and thoroughness of the tests. As a result, these test suites tend to exercise only a small subset of the program's functionality and may be inadequate for testing the changes in a program. To address this issue, we propose a novel approach called Behavioral Regression Testing (BERT). Given two versions of a program, BERT identifies behavioral differences between the two versions through dynamical analysis, in three steps. First, it generates a large number of test inputs that focus on the changed parts of the code. Second, it runs the generated test inputs on the old and new versions of the code and identifies differences in the tests' behavior. Third, it analyzes the identified differences and presents them to the developers. By focusing on a subset of the code and leveraging differential behavior, BERT can provide developers with more (and more detailed) information than traditional regression testing techniques. To evaluate BERT, we implemented it as a plug-in for Eclipse, a popular Integrated Development Environment, and used the plug-in to perform a preliminary study on two programs. The results of our study are promising, in that BERT was able to identify true regression faults in the programs.","2159-4848","978-1-4244-6436-4","10.1109/ICST.2010.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477091","Regression testing;software evolution;dynamic analysis","Automatic testing;Bit error rate;Vehicle crash testing;Software testing;Fault diagnosis;Costs;Performance evaluation;Software maintenance;Inspection;Writing","automatic test software;program testing;regression analysis;software fault tolerance;software maintenance;software quality","automated behavioral regression testing technique;regression faults;program functionality;program testing;dynamical analysis;BERT;Eclipse;integrated development environment;software maintenance","","38","3","20","","3 Jun 2010","","","IEEE","IEEE Conferences"
"Tedsuto: A General Framework for Testing Dynamic Software Updates","L. Pina; M. Hicks","Imperial Coll. London, London, UK; Univ. of Maryland, College Park, MD, USA","2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)","21 Jul 2016","2016","","","278","287","Dynamic software updating (DSU) is a technique for patching running programs, to fix bugs or add new features. DSU avoids the downtime of stop-and-restart updates, but creates new risks -- an incorrect or ill-timed dynamic update could result in a crash or misbehavior, defeating the whole purpose of DSU. To reduce such risks, dynamic updates should be carefully tested before they are deployed. This paper presents Tedsuto, a general testing framework for DSU, along with a concrete implementation of it for Rubah, a state-of-the-art Java-based DSU system. Tedsuto uses system-level tests developed for the old and new versions of the updateable software, and systematically tests whether a dynamic update might result in a test failure. Very often this process is fully automated, while in some cases (e.g., to test new-version functionality) some manual annotations are required. To evaluate Tedsuto's efficacy, we applied it to dynamic updates previously developed (and tested in an ad hoc manner) for the H2 SQL database server and the CrossFTP server -- two real-world, multithreaded systems. We used three large test suites, totalling 446 tests, and we found a variety of update-related bugs quickly, and at low cost.","","978-1-5090-1827-7","10.1109/ICST.2016.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515479","","Servers;Testing;Software;Databases;Computer bugs;Radiation detectors","Java;multi-threading;program debugging;program testing;SQL","Tedsuto;dynamic software updates testing;running program patching;stop-and-restart updates;Rubah Java-based DSU system;system-level tests;H2 SQL database server;CrossFTP server;multithreaded systems;update-related bugs","","8","","24","","21 Jul 2016","","","IEEE","IEEE Conferences"
"Extension-Aware Automated Testing Based on Imperative Predicates","N. Dini; C. Yelen; M. Gligoric; S. Khurshid",The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)","6 Jun 2019","2019","","","25","36","Bounded exhaustive testing (BET) techniques have been shown to be effective for detecting faults in software. BET techniques based on imperative predicates, enumerate all test inputs up to the given bounds such that each test input satisfies the properties encoded by the predicate. The search space is bounded by the user, who specifies the number of objects of each type and the list of values for each field of each type. To optimize the search, existing techniques detect isomorphic instances and record accessed fields during the execution of a predicate. However, these optimizations are extension-unaware, i.e., they do not speed up the search when the predicate is modified, say due to a fix or additional properties. We present a technique, named iGen, that speeds up test generation when imperative predicates are extended. iGen memoizes intermediate results of a test generation and reuses the results in a future search - even when the new search space differs from the old space. We integrated our technique in two BET tools (one for Java and one for Python) and evaluated these implementations with several data structure pairs, including two pairs from the Standard Java Library. Our results show that iGen speeds up test generation by up to 46.59x for the Java tool and up to 49.47x for the Python tool. Additionally, we show that the speedup obtained by iGen increases for larger test instances.","2159-4848","978-1-7281-1736-2","10.1109/ICST.2019.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730211","Bounded exhaustive testing;Imperative predicates;Korat;Alloy","Test pattern generators;Space exploration;Java;Binary search trees;Tools;Python","Java;program testing","imperative predicates;exhaustive testing techniques;test input;isomorphic instances;test generation;BET tools;extension-aware automated testing;iGen technique;bounded exhaustive testing;Java;Python;fault detection","","2","","59","","6 Jun 2019","","","IEEE","IEEE Conferences"
"Language-Agnostic Generation of Compilable Test Programs","P. Kreutzer; S. Kraus; M. Philippsen","Friedrich-Alexander University Erlangen-Nürnberg (FAU),Programming Systems Group,Germany; Friedrich-Alexander University Erlangen-Nürnberg (FAU),Programming Systems Group,Germany; Friedrich-Alexander University Erlangen-Nürnberg (FAU),Programming Systems Group,Germany","2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)","5 Aug 2020","2020","","","39","50","Testing is an integral part of the development of compilers and other language processors. To automatically create large sets of test programs, random program generators, or fuzzers, have emerged. Unfortunately, existing approaches are either language-specific (and thus require a rewrite for each language) or may generate programs that violate rules of the respective programming language (which limits their usefulness). This work introduces *Smith, a language-agnostic framework for the generation of valid, compilable test programs. It takes as input an abstract attribute grammar that specifies the syntactic and semantic rules of a programming language. It then creates test programs that satisfy all these rules. By aggressively pruning the search space and keeping the construction as local as possible, *Smith can generate huge, complex test programs in short time. We present four case studies covering four real-world programming languages (C, Lua, SQL, and SMT-LIB 2) to show that *Smith is both efficient and effective, while being flexible enough to support programming languages that differ considerably. We found bugs in all four case studies. For example, *Smith detected 165 different crashes in older versions of GCC and LLVM.","2159-4848","978-1-7281-5778-8","10.1109/ICST46399.2020.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159098","fuzz testing;compilers;attribute grammars","Grammar;Program processors;Production;Generators;Syntactics;Computer languages;Computer bugs","attribute grammars;automatic programming;computability;program compilers;program debugging;program testing;program verification;programming language semantics;programming languages;software architecture;SQL","language-agnostic generation;compilable test programs;language processors;random program generators;language-specific;respective programming language;*Smith;language-agnostic framework;valid test programs;syntactic;semantic rules;huge test programs;complex test programs;real-world programming languages","","1","","54","","5 Aug 2020","","","IEEE","IEEE Conferences"
