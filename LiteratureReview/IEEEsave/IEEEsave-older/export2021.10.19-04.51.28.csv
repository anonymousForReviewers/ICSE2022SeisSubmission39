"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Systematic Literature Review on Bad Smells–5 W's: Which, When, What, Who, Where","E. V. d. P. Sobrinho; A. De Lucia; M. d. A. Maia","Department of Electrical Engineering, Federal University of Triângulo Mineiro, Uberaba, Brazil; University of Salerno, Fisciano, (SA), Italy; Faculty of Computing, Federal University of Uberlândia, Uberlândia, Brazil","IEEE Transactions on Software Engineering","8 Jan 2021","2021","47","1","17","66","Bad smells are sub-optimal code structures that may represent problems needing attention. We conduct an extensive literature review on bad smells relying on a large body of knowledge from 1990 to 2017. We show that some smells are much more studied in the literature than others, and also that some of them are intrinsically inter-related (which). We give a perspective on how the research has been driven across time (when). In particular, while the interest in duplicated code emerged before the reference publications by Fowler and Beck and by Brown et al., other types of bad smells only started to be studied after these seminal publications, with an increasing trend in the last decade. We analyzed aims, findings, and respective experimental settings, and observed that the variability of these elements may be responsible for some apparently contradictory findings on bad smells (what). Moreover, we could observe that, in general, papers tend to study different types of smells at once. However, only a small percentage of those papers actually investigate possible relations between the respective smells (co-studies), i.e., each smell tends to be studied in isolation. Despite of a few relations between some types of bad smells have been investigated, there are other possible relations for further investigation. We also report that authors have different levels of interest in the subject, some of them publishing sporadically and others continuously (who). We observed that scientific connections are ruled by a large “small world” connected graph among researchers and several small disconnected graphs. We also found that the communities studying duplicated code and other types of bad smells are largely separated. Finally, we observed that some venues are more likely to disseminate knowledge on Duplicate Code (which often is listed as a conference topic on its own), while others have a more balanced distribution among other smells (where). Finally, we provide a discussion on future directions for bad smell research.","1939-3520","","10.1109/TSE.2018.2880977","Fundação de Amparo à Pesquisa do Estado de Minas Gerais; Conselho Nacional de Desenvolvimento Científico e Tecnológico; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8532309","Software maintenance;reengineering;bad smell","Systematics;Bibliographies;Software;Measurement;Organizations;Tools;Cloning","graph theory;program diagnostics;software maintenance","systematic literature review;bad smells;duplicated code;possible relations;bad smell research;suboptimal code structures;small world connected graph","","2","","417","IEEE","11 Nov 2018","","","IEEE","IEEE Journals"
"Requirements elicitation and validation with real world scenes","P. Haumer; K. Pohl; K. Weidenhaupt","Tech. Hochschule Aachen, Germany; NA; NA","IEEE Transactions on Software Engineering","6 Aug 2002","1998","24","12","1036","1054","A requirements specification defines the requirements for the future system at a conceptual level (i.e., class or type level). In contrast, a scenario represents a concrete example of current or future system usage. In early RE phases, scenarios are used to support the definition of high level requirements (goals) to be achieved by the new system. In many cases, those goals can to a large degree be elicited by observing, documenting and analyzing scenarios about current system usage. To support the elicitation and validation of the goals achieved by the existing system and to illustrate problems of the old system, we propose to capture current system usage using rich media (e.g., video, speech, pictures, etc.) and to interrelate those observations with the goal definitions. Thus, we aim at making the abstraction process which leads to the definition of the conceptual models more transparent and traceable. We relate the parts of the observations which have caused the definition of a goal or against which a goal was validated with the corresponding goal. These interrelations provide the basis for: 1) explaining and illustrating a goal model to, e.g., untrained stakeholders and/or new team members; 2) detecting, analyzing, and resolving a different interpretation of the observations; 3) comparing different observations using computed goal annotations; and 4) refining or detailing a goal model during later process phases. Using the PRIME implementation framework, we have implemented the PRIME-CREWS environment, which supports the interrelation of conceptual models and captured system usage observations. We report on our experiences with PRIME-CREWS gained in an experimental case study.","1939-3520","","10.1109/32.738338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738338","","Layout;Object oriented modeling;Context modeling;History;Erbium;Unified modeling language;Business process re-engineering;System testing;Knowledge engineering;Concrete","formal specification;program verification;systems analysis;program diagnostics","requirements elicitation;requirements validation;real world scenes;requirements specification;future system;conceptual level;type level;future system usage;early RE phases;high level requirements;new system;current system usage;old system;goal definitions;abstraction process;conceptual models;untrained stakeholders;new team members;computed goal annotations;goal model;PRIME implementation framework;PRIME-CREWS environment;captured system usage observations;experimental case study","","85","","48","","6 Aug 2002","","","IEEE","IEEE Journals"
"Perturbation techniques for detecting domain errors","S. J. Zeil","Dept. of Comput. Sci., Old Dominion Univ., Norfolk, VA, USA","IEEE Transactions on Software Engineering","6 Aug 2002","1989","15","6","737","746","Perturbation testing is an approach to software testing which focuses on faults within arithmetic expressions appearing throughout a program. This approach is expanded to permit analysis of individual test points rather than entire paths, and to concentrate on domain errors. Faults are modeled as perturbing functions drawn from a vector space of potential faults and added to the correct form of an arithmetic expression. Sensitivity measures are derived which limit the possible size of those faults that would go undetected after the execution of a given test set. These measures open up an interesting view of testing, in which attempts are made to reduce the volume of possible faults which, were they present in the program being tested, would have escaped detection on all tests performed so far. The combination of these measures with standard optimization techniques yields a novel test-data-generation method called arithmetic fault detection.<<ETX>></ETX>","1939-3520","","10.1109/32.24727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=24727","","Perturbation methods;Software testing;Error correction;Fault detection;Size measurement;Volume measurement;Measurement standards;Optimization methods;Floating-point arithmetic;Computer science","error detection;perturbation techniques;program testing","error detection;sensitivity measures;software testing;arithmetic expressions;individual test points;domain errors;perturbing functions;vector space;potential faults;test set;standard optimization techniques;novel test-data-generation method;arithmetic fault detection","","29","","19","","6 Aug 2002","","","IEEE","IEEE Journals"
"Modified rate-monotonic algorithm for scheduling periodic jobs with deferred deadlines","Wei Kuan Shih; J. W. S. Liu; C. L. Liu","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","6 Aug 2002","1993","19","12","1171","1179","The deadline of a request is the time instant at which its execution must complete. The deadline of the request in any period of a job with deferred deadline is some time instant after the end of the period. The authors describe a semi-static priority-driven algorithm for scheduling periodic jobs with deferred deadlines: each job is assigned two priorities, the higher one for old requests and the lower one for the current request. This algorithm is called the modified rate-monotonic algorithm and is based on the well-known rate-monotonic algorithm. It is shown that the modified rate-monotonic algorithm is optimal when the deadline of every job is deferred by max (1, gamma -1) periods or more, where gamma is the ratio between the longest period and the shortest period. When the deadline of each job is deferred by one period of the job, any set of n independent jobs whose total utilization is equal to or less than (1+n(2/sup 1/n/-1))/2 can be feasibly scheduled by this algorithm. This bound approaches 0.845 when n approaches infinity.<<ETX>></ETX>","1939-3520","","10.1109/32.249662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=249662","","Scheduling algorithm;Processor scheduling;Real time systems;H infinity control;Operating systems;Aerospace control;Computer science","computational complexity;multiprogramming;operating systems (computers);real-time systems;scheduling","modified rate-monotonic algorithm;periodic jobs;deferred deadlines;semi-static priority-driven algorithm;time instant;old requests;current request;independent jobs;feasibly scheduled;job scheduling;request deadline;deterministic scheduling theory;embedded systems;operating system;real-time systems;scheduling algorithms","","25","1","8","","6 Aug 2002","","","IEEE","IEEE Journals"
"Semantics guided regression test cost reduction","D. Binkley","Dept. of Comput. Sci., Loyola Coll., Baltimore, MD, USA","IEEE Transactions on Software Engineering","6 Aug 2002","1997","23","8","498","516","Software maintainers are faced with the task of regression testing: retesting a modified program on an often large number of test cases. The cost of regression testing can be reduced if the size of the program is reduced and if old test cases and results can be reused. Two complimentary algorithms for reducing the cost of regression testing are presented. The first produces a program called Differences that captures the semantic change between Certified, a previously tested program, and Modified, a changed version of Certified. It is more efficient to test Differences, because it omits unchanged computations. The program Differences is computed using a combination of program slices. The second algorithm identifies test cases for which Certified and Modified produce the same output and existing test cases that test new components in Modified. The algorithm is based on the notion of common execution patterns. Program components with common execution patterns have the same execution pattern during some call to their procedure. They are computed using a calling context slice. Whereas an interprocedural slice includes the program components necessary to capture all possible executions of a statement, a calling context slice includes only those program components necessary to capture the execution of a statement in a particular calling context. Together with Differences, it is possible to test Modified by running Differences on a smaller number of test cases. This is more efficient than running Modified on a large number of test cases. A prototype implementation has been built to examine and illustrate these algorithms.","1939-3520","","10.1109/32.624306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624306","","Costs;Automatic testing;Software testing;Software debugging;Software maintenance;Software tools;Computer Society;Software prototyping;Prototypes;Software engineering","program testing;software maintenance;subroutines;software cost estimation","semantics-guided regression test cost reduction;software maintenance;modified program retesting;old test cases;semantic change;program differences;certified program;program slices;common execution patterns;procedure calls;calling context slice;interprocedural slice;program components;program statement executions","","121","2","43","","6 Aug 2002","","","IEEE","IEEE Journals"
"Predicting fault incidence using software change history","T. L. Graves; A. F. Karr; J. S. Marron; H. Siy","Los Alamos Nat. Lab., NM, USA; NA; NA; NA","IEEE Transactions on Software Engineering","6 Aug 2002","2000","26","7","653","661","This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight.","1939-3520","","10.1109/32.859533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859533","","History;Predictive models;Software systems;Aging;Time measurement;Software measurement;Length measurement;Software development management;Statistical analysis;Degradation","software maintenance;software metrics;software fault tolerance;management of change","fault incidence;software change history;change management data;change history;fault potential;code decay;metrics;statistical analysis","","401","12","21","","6 Aug 2002","","","IEEE","IEEE Journals"
"Discarding Obsolete Information in a Replicated Database System","S. K. Sarin; N. A. Lynch",Computer Corporation of America; NA,"IEEE Transactions on Software Engineering","18 Sep 2006","1987","SE-13","1","39","47","A replicated database architecture is described in which updates processed at a site must be saved to allow reconcilliation of newly arriving updates in a way that preserves mutual consistency. The storage space occupied by the saved updates increases indefinitely, and periodic discarding of old updates is needed to avoid running out of storage. A protocol is described which allows sites in the system to agree that updates older than a given timestamp are no longer needed and can be discarded. This protocol uses a ""distributed snapshot"" algorithm of Chandy and Lamport and represents a practical application of that algorithm. A protocol for permanent removal of sites is also described, which will allow the discarding of updates to continue when one or more sites crash and are expected not to recover.","1939-3520","","10.1109/TSE.1987.232564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702131","Distributed databases;distributed snapshots;mutual consistency;network partitions;replicated data;timestamps","Database systems;Protocols;Pipelines;Partitioning algorithms;Computer crashes;Distributed databases;Availability;Government;Computer science;Protection","","Distributed databases;distributed snapshots;mutual consistency;network partitions;replicated data;timestamps","","31","18","9","","18 Sep 2006","","","IEEE","IEEE Journals"
"Distributed Version Management for Read-Only Actions","W. E. Weihl","M. I. T. Laboratory for Computer Science, 545 Technology Square","IEEE Transactions on Software Engineering","18 Sep 2006","1987","SE-13","1","55","64","Typical concurrency control protocols for atomic actions, such as two-phase locking, perform poorly for long read-only actions. We present four new concurrency control protocols that eliminate all interference between read-only actions and update actions, and thus offer significantly improved performance for read-only actions. The protocols work by maintaining multiple versions of the system state; read-only actions read old versions, while update actions manipulate the most recent version. We focus on the problem of managing the storage required for old versions in a distributed system. One of the protocols uses relatively little space, but has a potentially significant communication cost. The other protocols use more space, but may be cheaper in terms of communication.","1939-3520","","10.1109/TSE.1987.232835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702133","Atomic actions;concurrency;distributed systems;multiple version protocols;read-only actions;storage management","Protocols;Concurrency control;Hardware;Interference elimination;Costs;Concurrent computing;Data mining;Application software;Delay effects;System recovery","","Atomic actions;concurrency;distributed systems;multiple version protocols;read-only actions;storage management","","22","14","30","","18 Sep 2006","","","IEEE","IEEE Journals"
"Whence to Learn? Transferring Knowledge in Configurable Systems using BEETLE","R. Krishna; V. Nair; P. Jamshidi; T. Menzies","Computer Science, Columbia University, 5798 New York, New York United States (e-mail: i.m.ralk@gmail.com); Computer Science, NC State University, 6798 Raleigh, North Carolina United States (e-mail: vivekaxl@gmail.com); Computer Science and Engineering, University of South Carolina, Columbia, South Carolina United States (e-mail: pjamshid@cse.sc.edu); Computer Science, North Carolina State University College of Engineering, 242510 Raleigh, North Carolina United States 27695-7901 (e-mail: timm@ieee.org)","IEEE Transactions on Software Engineering","","2020","PP","99","1","1","As software systems grow in complexity and the space of possible configurations increases exponentially, finding the near-optimal configuration of a software system becomes challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, collecting enough sample configurations can be very expensive since each such sample requires configuring, compiling, and executing the entire system using a complex test suite. When learning on new data is too expensive, it is possible to use Transfer Learning to “transfer” old lessons to the new context. Traditional transfer learning has a number of challenges, specifically, (a) learning from excessive data takes excessive time, and (b) the performance of the models built via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer learning framework called BEETLE, which is a “bellwether”-based transfer learner that focuses on identifying and learning from the most relevant source from amongst the old data. This paper evaluates BEETLE with 57 different software configuration problems based on five software systems (a video encoder, an SAT solver, a SQL database, a high-performance C-compiler, and a streaming data analytics tool). In each of these cases, BEETLE found configurations that are as good as or better than those found by other state-of-the-art transfer learners while requiring only a fraction 1/7th of the measurements needed by those other methods. Based on these results, we say that BEETLE is a new high-water mark in optimally configuring software.","1939-3520","","10.1109/TSE.2020.2983927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050841","Performance Optimization;SBSE;Transfer Learning;Bellwether","Optimization;Software systems;Data collection;Tools;Computer science;Software engineering","","","","","","","IEEE","30 Mar 2020","","","IEEE","IEEE Early Access Articles"
"Design recovery for distributed systems","L. J. Holtzblatt; R. L. Piazza; H. B. Reubenstein; S. N. Roberts; D. R. Harris","Reasoning Inc., Mountain View, CA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","6 Aug 2002","1997","23","7","461","472","Two factors limit the utility of reverse engineering technology for many distributed software systems. First, with the exception of tools that support Ada and its explicit tasking constructs, reverse engineering tools fail to capture information concerning the flow of information between tasks. Second, relatively few reverse engineering tools are available for programming languages in which many older legacy applications were written (e.g., Jovial, CMS-2, and various assembly languages). We describe approaches that were developed for overcoming these limitations. In particular, we have implemented an approach for automatically extracting task flow information from a command and control system written in CMS-2. Our approach takes advantage of a small amount of externally provided design knowledge in order to recover design information relevant to the distributed nature of the target system.","1939-3520","","10.1109/32.605763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=605763","","Software maintenance;Reverse engineering;Software systems;Data mining;Navigation;Productivity;Displays;Software tools;Computer languages;Application software","distributed processing;reverse engineering;software tools;Ada;command and control systems","design recovery;distributed systems;reverse engineering;tools;Ada;tasking constructs;information flow;programming languages;legacy applications;Jovial;CMS-2;assembly languages;command and control system;program understanding","","22","","32","","6 Aug 2002","","","IEEE","IEEE Journals"
"On the Semantics of Associations and Association Ends in UML","D. Milicev",IEEE Computer Society,"IEEE Transactions on Software Engineering","12 Mar 2007","2007","33","4","238","251","Association is one of the key concepts in UML that is intensively used in conceptual modeling. Unfortunately, in spite of the fact that this concept is very old and is inherited from other successful modeling techniques, a fully unambiguous understanding of it, especially in correlation with other newer concepts connected with association ends, such as uniqueness, still does not exist. This paper describes a problem with one widely assumed interpretation of the uniqueness of association ends, the restrictive interpretation, and proposes an alternative, the intentional interpretation. Instead of restricting the association from having duplicate links, uniqueness of an association end in the intentional interpretation modifies the way in which the association end maps an object of the opposite class to a collection of objects of the class at that association end. If the association end is unique, the collection is a set obtained by projecting the collection of all linked objects. In that sense, the uniqueness of an association end modifies the view to the objects at that end, but does not constrain the underlying object structure. This paper demonstrates how the intentional interpretation improves expressiveness of the modeling language and has some other interesting advantages. Finally, this paper gives a completely formal definition of the concepts of association and association ends, along with the related notions of uniqueness, ordering, and multiplicity. The semantics of the UML actions on associations are also defined formally","1939-3520","","10.1109/TSE.2007.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123326","Object-oriented modeling;Unified Modeling Language (UML);association;association end;formal semantics;conceptual modeling;model-driven development.","Unified modeling language;Object oriented modeling;Application software;Switches;Erbium;Computer Society;Visualization;Software systems;Computer industry;Object oriented databases","entity-relationship modelling;formal specification;object-oriented programming;programming language semantics;Unified Modeling Language","UML;conceptual modeling;intentional interpretation;association end;object-oriented modeling;Unified Modeling Language;formal semantics;formal specification","","13","","21","","12 Mar 2007","","","IEEE","IEEE Journals"
"Sufficient condition for a communication deadlock and distributed deadlock detection","B. E. Wojcik; Z. M. Wojcik","Beechcraft Co., Wichita, KS, USA; NA","IEEE Transactions on Software Engineering","6 Aug 2002","1989","15","12","1587","1595","The necessary and sufficient condition for deadlock in a distributed system and an algorithm for detection of a distributed deadlock based on the sufficient condition are formulated. The protocol formulated, checks all wait-for contiguous requests in one iteration. A cycle is detected when a query message reaches the initiator. A wait-for cycle is only the necessary condition for the distributed deadlock. A no-deadlock message is expected by the query initiator to infer a deadlock-free situation if at least one wait-for cycle is present. A no-deadlock message is issued by a dependent (query intercessor) that is not waiting-for. No no-deadlock message implies a deadlock, and processes listed in the received query messages are the processes involved in a distributed deadlock. Properties of the protocol are discussed. The authors show that a replication of a requested higher-priority (or older) process can prevent a distributed deadlock (in a continuous deadlock treatment). A replication is shown to recover (in a periodical deadlock handling) a sequence of processes from an indefinite wait-die scheme.<<ETX>></ETX>","1939-3520","","10.1109/32.58770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=58770","","System recovery;Sufficient conditions;Protocols;Network servers;Resource management;Imaging phantoms;Mathematics;Computer science;Statistics;Hardware","distributed processing;system recovery","communication deadlock;distributed deadlock detection;sufficient condition;protocol;wait-for contiguous requests;query message;wait-for cycle;no-deadlock message;query initiator;deadlock-free situation;query intercessor;replication;periodical deadlock handling;indefinite wait-die scheme","","8","","12","","6 Aug 2002","","","IEEE","IEEE Journals"
"Coverage Prediction for Accelerating Compiler Testing","J. Chen; G. Wang; D. Hao; Y. Xiong; H. Zhang; L. Zhang; B. Xie","Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China; University of Newcastle, Newcastle, NSW, Australia; Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China","IEEE Transactions on Software Engineering","11 Feb 2021","2021","47","2","261","278","Compilers are one of the most fundamental software systems. Compiler testing is important for assuring the quality of compilers. Due to the crucial role of compilers, they have to be well tested. Therefore, automated compiler testing techniques (those based on randomly generated programs) tend to run a large number of test programs (which are test inputs of compilers). The cost for compilation and execution for these test programs is significant. These techniques can take a long period of testing time to detect a relatively small number of compiler bugs. That may cause many practical problems, e.g., bringing a lot of costs including time costs and financial costs, and delaying the development/release cycle. Recently, some approaches have been proposed to accelerate compiler testing by executing test programs that are more likely to trigger compiler bugs earlier according to some criteria. However, these approaches ignore an important aspect in compiler testing: different test programs may have similar test capabilities (i.e., testing similar functionalities of a compiler, even detecting the same compiler bug), which may largely discount their acceleration effectiveness if the test programs with similar test capabilities are executed all the time. Test coverage is a proper approximation to help distinguish them, but collecting coverage dynamically is infeasible in compiler testing since most test programs are generated on the fly by automatic test-generation tools like Csmith. In this paper, we propose the first method to predict test coverage statically for compilers, and then propose to prioritize test programs by clustering them according to the predicted coverage information. The novel approach to accelerating compiler testing through coverage prediction is called COP (short for COverage Prediction). Our evaluation on GCC and LLVM demonstrates that COP significantly accelerates compiler testing, achieving an average of 51.01 percent speedup in test execution time on an existing dataset including three old release versions of the compilers and achieving an average of 68.74 percent speedup on a new dataset including 12 latest release versions. Moreover, COP outperforms the state-of-the-art acceleration approach significantly by improving <inline-formula><tex-math notation=""LaTeX"">$17.16\%\sim 82.51\%$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mrow><mml:mn>17</mml:mn><mml:mo>.</mml:mo><mml:mn>16</mml:mn><mml:mo>%</mml:mo><mml:mo>∼</mml:mo><mml:mn>82</mml:mn><mml:mo>.</mml:mo><mml:mn>51</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""chen-ieq1-2889771.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> speedups in different settings on average.","1939-3520","","10.1109/TSE.2018.2889771","National Key Research and Development Program of China(grant numbers:2017YFB1001803); National Natural Science Foundation of China(grant numbers:61672047,61529201,61872008,61828201,61672045,61861130363); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588375","Compiler testing;test prioritization;machine learning","Testing;Program processors;Computer bugs;Life estimation;Acceleration;Optimization;Electromagnetic interference","","","","3","","86","IEEE","25 Dec 2018","","","IEEE","IEEE Journals"
"Dynamic Update of Discrete Event Controllers","L. Nahabedian; V. Braberman; N. D'Ippolito; S. Honiden; J. Kramer; K. Tei; S. Uchitel","FCEyN, ICC, CONICET, Universidad de Buenos Aires, Buenos Aires, Argentina; FCEyN, ICC, CONICET, Universidad de Buenos Aires, Buenos Aires, Argentina; FCEyN, ICC, CONICET, Universidad de Buenos Aires, Buenos Aires, Argentina; National Institute of Informatics, Tokyo, Japan; Department of Computing, Imperial College, London, United Kingdom; National Institute of Informatics, Tokyo, Japan; FCEyN, ICC, CONICET, Universidad de Buenos Aires, Buenos Aires, Argentina","IEEE Transactions on Software Engineering","13 Nov 2020","2020","46","11","1220","1240","Discrete event controllers are at the heart of many software systems that require continuous operation. Changing these controllers at runtime to cope with changes in its execution environment or system requirements change is a challenging open problem. In this paper we address the problem of dynamic update of controllers in reactive systems. We present a general approach to specifying correctness criteria for dynamic update and a technique for automatically computing a controller that handles the transition from the old to the new specification, assuring that the system will reach a state in which such a transition can correctly occur and in which the underlying system architecture can reconfigure. Our solution uses discrete event controller synthesis to automatically build a controller that guarantees both progress towards update and safe update.","1939-3520","","10.1109/TSE.2018.2876843","ANPCYT; Secretaría de Ciencia y Técnica, Universidad de Buenos Aires; Consejo Nacional de Investigaciones Científicas y Técnicas; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500345","Controller synthesis;dynamic update;adaptive systems","Tools;Runtime;Paints;Control systems;Business;Safety","control system synthesis;discrete event systems","discrete event controllers;software systems;reactive systems;event controller synthesis","","2","","71","IEEE","19 Oct 2018","","","IEEE","IEEE Journals"
"An Empirical Study of Dependency Downgrades in the npm Ecosystem","F. R. Cogo; G. A. Oliva; A. E. Hassan","School of Computing, Queen's University, 4257 Kingston, Ontario Canada (e-mail: filipe.cogo@gmail.com); School of Computing, Queen's University, Kingston, Ontario Canada K7L 2N8 (e-mail: golivax@gmail.com); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","In a software ecosystem, a dependency relationship enables a client package to reuse a certain version of a provider package. Packages in a software ecosystem often release versions containing bug fixes, new functionalities, and security enhancements. Hence, updating the provider version is an important maintenance task for client packages. Despite the number of investigations about dependency updates, there is a lack of studies about dependency downgrades in software ecosystems. A downgrade indicates that the adopted version of a provider package is not suitable to the client package at a certain moment. In this paper, we investigate downgrades in the npm ecosystem. We address three research questions. In our first RQ, we provide a list of the reasons behind the occurrence of downgrades. Two categories of downgrades according to their rationale: reactive and preventive. The reasons behind reactive downgrades are defects in a specific version of a provider, unexpected feature changes in a provider, and incompatibilities. In turn, preventive downgrades are an attempt to avoid issues in future releases. In our second RQ, we investigate how the versioning of dependencies is modified when a downgrade occurs. We observed that 49% of the downgrades are performed by replacing a range of acceptable versions of a provider by a specific old version. Also, 48% of the downgrades reduce the provider version by a minor level (e.g., from 2.1.0 to 2.0.0). In our third RQ we observed that 50% of the downgrades are performed at a rate that is four times as slow as the median time-between-releases of their associated client packages. We also observed that downgrades that follow an explicit update of a provider package occur 9 times faster than downgrades that follow an implicit update. Explicit updates occur when the provider is updated by means of an explicit change to the versioning specification (i.e., the string used by client packages to define the provider version that they are willing to adopt). We conjecture that, due to the controlled nature of explicit updates, it is easier for client packages to identify the provider that is associated with the problem that motivated the downgrade.","1939-3520","","10.1109/TSE.2019.2952130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894401","downgrades;dependency management;npm;software ecosystems","Ecosystems;Software;Computer bugs;Tools;Security;Task analysis","","","","2","","","IEEE","8 Nov 2019","","","IEEE","IEEE Early Access Articles"
"Dynamic Software Updating Using a Relaxed Consistency Model","H. Chen; J. Yu; C. Hang; B. Zang; P. Yew","Fudan University, Shanghai; University of Michigan, Ann Arbor; Microsoft (China) Ltd., Shanghai; Fudan University, Shanghai; University of Minnesota at Twin Cities, Minneapolis","IEEE Transactions on Software Engineering","29 Sep 2011","2011","37","5","679","694","Software is inevitably subject to changes. There are patches and upgrades that close vulnerabilities, fix bugs, and evolve software with new features. Unfortunately, most traditional dynamic software updating approaches suffer some level of limitations; few of them can update multithreaded applications when involving data structure changes, while some of them lose binary compatibility or incur nonnegligible performance overhead. This paper presents POLUS, a software maintenance tool capable of iteratively evolving running unmodified multithreaded software into newer versions, yet with very low performance overhead. The main idea in POLUS is a relaxed consistency model that permits the concurrent activity of the old and new code. POLUS borrows the idea of cache-coherence protocol in computer architecture and uses a ”bidirectional write-through” synchronization protocol to ensure system consistency. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd, and Apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead on the three applications-a less than 1 percent performance degradation (but 5 percent for one case). The time to apply an update is also minimal.","1939-3520","","10.1109/TSE.2010.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551162","Maintainability;reliability;runtime environments.","Software;Synchronization;Protocols;Bidirectional control;Registers;Runtime","computer architecture;hypermedia;multi-threading;program testing;software maintenance;software tools;transport protocols","dynamic software update;relaxed consistency model;data structure;binary compatibility;nonnegligible performance;POLUS;software maintenance tool;iteratively evolving running unmodified multithreaded software;concurrent activity;cache-coherence protocol;computer architecture;bidirectional write-through synchronization protocol;prevalent server application;HTTP server","","27","4","42","","19 Aug 2010","","","IEEE","IEEE Journals"
"Managing requirements inconsistency with development goal monitors","W. N. Robinson; S. D. Pawlowski","Dept. of Comput. Inf. Syst., Georgia State Univ., Atlanta, GA, USA; NA","IEEE Transactions on Software Engineering","6 Aug 2002","1999","25","6","816","835","Managing the development of software requirements can be a complex and difficult task. The environment is often chaotic. As analysts and customers leave the project, they are replaced by others who drive development in new directions. As a result, inconsistencies arise. Newer requirements introduce inconsistencies with older requirements. The introduction of such requirements inconsistencies may violate stated goals of development. In this article, techniques are presented that manage requirements document inconsistency by managing inconsistencies that arise between requirement development goals and requirements development enactment. A specialized development model, called a requirements dialog meta-model, is presented. This meta-model defines a conceptual framework for dialog goal definition, monitoring, and in the case of goal failure, dialog goal reestablishment. The requirements dialog meta-model is supported in an automated multiuser World Wide Web environment, called DEALSCRIBE. An exploratory case study of its use is reported. This research supports the conclusions that: an automated tool that supports the dialog meta-model can automate the monitoring and reestablishment of formal development goals; development goal monitoring can be used to determine statements of a development dialog that fail to satisfy development goals; and development goal monitoring can be used to manage inconsistencies in a developing requirements document. The application of DEALSCRIBE demonstrates that a dialog meta-model can enable a powerful environment for managing development and document inconsistencies.","1939-3520","","10.1109/32.824411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=824411","","Computerized monitoring;Condition monitoring;Power system management;Design engineering;Power engineering and energy;Computer aided software engineering;Software development management;Student members;Chaos;Web sites","systems analysis;formal specification;software development management;information resources","requirements inconsistency management;development goal monitors;software requirements development management;requirement development goals;requirements development enactment;requirements dialog meta-model;dialog goal definition;World Wide Web;multiuser environment;DEALSCRIBE","","41","","75","","6 Aug 2002","","","IEEE","IEEE Journals"
"A Screening Test for Disclosed Vulnerabilities in FOSS Components","S. Dashevskyi; A. D. Brucker; F. Massacci","University of Trento, TN, Italy; University of Sheffield, Sheffield, United Kingdom; University of Trento, Trento, TN, Italy","IEEE Transactions on Software Engineering","16 Oct 2019","2019","45","10","945","966","Free and Open Source Software (FOSS) components are ubiquitous in both proprietary and open source applications. Each time a vulnerability is disclosed in a FOSS component, a software vendor using this component in an application must decide whether to update the FOSS component, patch the application itself, or just do nothing as the vulnerability is not applicable to the older version of the FOSS component used. This is particularly challenging for enterprise software vendors that consume thousands of FOSS components and offer more than a decade of support and security fixes for their applications. Moreover, customers expect vendors to react quickly on disclosed vulnerabilities-in case of widely discussed vulnerabilities such as Heartbleed, within hours. To address this challenge, we propose a screening test: a novel, automatic method based on thin slicing, for estimating quickly whether a given vulnerability is present in a consumed FOSS component by looking across its entire repository. We show that our screening test scales to large open source projects (e.g., Apache Tomcat, Spring Framework, Jenkins) that are routinely used by large software vendors, scanning thousands of commits and hundred thousands lines of code in a matter of minutes. Further, we provide insights on the empirical probability that, on the above mentioned projects, a potentially vulnerable component might not actually be vulnerable after all.","1939-3520","","10.1109/TSE.2018.2816033","European Commission(grant numbers:317387); CISCO Country Digitalization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8316943","Security maintenance;security vulnerabilities;patch management;free and open source software","Security;Maintenance engineering;Tools;Jacobian matrices;Patents;Open source software","DP industry;program slicing;program testing;public domain software;security of data","proprietary source applications;open source applications;screening test;disclosed vulnerabilities;FOSS component;free and open source software component;thin slicing;enterprise software vendors","","2","","51","IEEE","15 Mar 2018","","","IEEE","IEEE Journals"
"Site Initialization, Recovery, and Backup in a Distributed Database System","R. Attar; P. A. Bernstein; N. Goodman","Department of Research and Development, Computer Division, Israeli Discount Bank.; Sequoia Systems, Inc., Marlborough, MA 01752.; Sequoia Systems, Inc., Marlborough, MA 01752.","IEEE Transactions on Software Engineering","29 May 2009","1984","SE-10","6","645","650","Site initialization is the problem of integrating a new site into a running distributed database system (DDBS). Site recovery is the problem of integrating an old site into a DDBS when the site recovers from failure. Site backup is the problem of creating a static backup copy of a database for archival or query purposes. We present an algorithm that solves the site initialization problem. By modifying the algorithm slightly, we get solutions to the other two problems as well. Our algorithm exploits the fact that a correct DDBS must run a serializable concurrency control algorithm. Our algorithm relies on the concurrency control algorithm to handle all intersite synchronization.","1939-3520","","10.1109/TSE.1984.5010293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010293","Distributed database systems;fault recovery;site initialization","Database systems;Concurrency control;Transaction databases;Contracts;Concurrent computing;Research and development","","","","30","8","16","","29 May 2009","","","IEEE","IEEE Journals"
"Checking inside the black box: regression testing by comparing value spectra","T. Xie; D. Notkin","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","21 Nov 2005","2005","31","10","869","883","Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Black-box program outputs have been used to characterize program behaviors and they are compared over program versions in traditional regression testing. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavioral differences between versions. In this paper, we present a new class of program spectra, value spectra, that enriches the existing program spectra family. We compare the value spectra of a program's old version and new version to detect internal behavioral deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavioral deviations. We also use path spectra (previously proposed program spectra) to approximate the program states in value spectra. We then similarly compare path spectra to detect behavioral deviations and locate deviation roots in the new version. We have conducted an experiment on eight C programs to evaluate our spectra-comparison approach. The results show that both value-spectra-comparison and path-spectra-comparison approaches can effectively expose program behavioral differences between program versions even when their program outputs are the same, and our value-spectra-comparison approach reports deviation roots with high accuracy for most programs.","1939-3520","","10.1109/TSE.2005.107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542068","Index Terms- Program spectra;regression testing;software testing;empirical studies;software maintenance.","Software testing;Software maintenance;Fault diagnosis;Propagation losses;Program processors;Optimizing compilers","program testing;software maintenance;formal verification;regression analysis","regression testing;program version;software maintenance;black-box program;program behavior;program spectra;deviation-propagation call tree;C program;value-spectra-comparison;path-spectra-comparison","","34","1","36","","21 Nov 2005","","","IEEE","IEEE Journals"
"Automating Live Update for Generic Server Programs","C. Giuffrida; C. Iorgulescu; G. Tamburrelli; A. S. Tanenbaum","Vrije Universiteit Amsterdam, De Boelelaan, Amsterdam, Netherlands; École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Vrije Universiteit Amsterdam, De Boelelaan, Amsterdam, Netherlands; Vrije Universiteit Amsterdam, De Boelelaan, Amsterdam, Netherlands","IEEE Transactions on Software Engineering","14 Mar 2017","2017","43","3","207","225","The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents <italic>Mutable Checkpoint-Restart</italic> (<italic>MCR</italic>), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (<italic>Apache httpd</italic>, <italic>nginx</italic>, <italic>OpenSSH</italic> and <italic>vsftpd</italic>) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9<inline-formula><tex-math notation=""LaTeX"">$\times$ </tex-math><alternatives><inline-graphic xlink:href=""giuffrida-ieq1-2584066.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> on average, without optimizations).","1939-3520","","10.1109/TSE.2016.2584066","European Research Council; ERC(grant numbers:227874); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497481","Live update;DSU;checkpoint-restart;quiescence detection;record-replay;garbage collection","Servers;Data structures;Convergence;Software;Manuals;System recovery;Buildings","","","","5","","57","IEEE","22 Jun 2016","","","IEEE","IEEE Journals"
"An Algorithm to Decide Feasibility of Linear Integer Constraints Occurrng in Decision Tables","S. Biswas; V. Rajaraman","Department of Computer Science and Engineering, Indian Institute of Technology; NA","IEEE Transactions on Software Engineering","18 Sep 2006","1987","SE-13","12","1340","1347","To detect errors in decision tables one needs to decide whether a given set of constraints is feasible or not. This paper describes an algorithm to do so when the constraints are linear in variables that take only integer values. Decision tables with such constraints occur frequently in business data processing and in nonnumeric applications. The aim of the algorithm is to exploit. the abundance of very simple constraints that occur in typical decision table contexts. Essentially, the algorithm is a backtrack procedure where the the solution space is pruned by using the set of simple constrains. After some simplications, the simple constraints are captured in an acyclic directed graph with weighted edges. Further, only those partial vectors are considered from extension which can be extended to assignments that will at least satisfy the simple constraints. This is how pruning of the solution space is achieved. For every partial assignment considered, the graph representation of the simple constraints provides a lower bound for each variable which is not yet assigned a value. These lower bounds play a vital role in the algorithm and they are obtained in an efficient manner by updating older lower bounds. Our present algorithm also incorporates an idea by which it can be checked whether or not an (m –2)-ary vector can be extended to a solution vector of m components, thereby backtracking is reduced by one component.","1939-3520","","10.1109/TSE.1987.233144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702183","Backtrack algorithm;decision tables;detection of errors;integer programming","Testing;Data processing;Linear programming;Calculus;Computer science;Integer linear programming","","Backtrack algorithm;decision tables;detection of errors;integer programming","","1","","12","","18 Sep 2006","","","IEEE","IEEE Journals"
"Structured Fortran—An Evolution of Standard Fortran","A. Ralston; J. L. Wagener","Department of Computer Science, State University of New York at Buffalo; NA","IEEE Transactions on Software Engineering","18 Sep 2006","1976","SE-2","3","154","176","Fortran is 21 years old; many would say that it has not reached adulthood but senility. Yet it remains the language of use in the overwhelming majority of scientific applications of computers and no end to this situation is in sight. Moreover, its evolution through the standards process is very slow, too slow to give any hope that Fortran will ever in this way become a language in which structured programs can be easily and effectively written.","1939-3520","","10.1109/TSE.1976.233813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702362","Control structures;data structures;data types;Fortran extensions;procedure-oriented languages;structured programing","Natural languages;Computer languages;Surgery;Computer science;Application software;Computer applications;Vehicles;Genetic programming;Data structures;Standards publication","","Control structures;data structures;data types;Fortran extensions;procedure-oriented languages;structured programing","","1","","23","","18 Sep 2006","","","IEEE","IEEE Journals"
"Application of Principal Component Analysis to Multikey Searching","R. C. T. Lee; Y. H. Chin; S. C. Chang","Institute of Applied Mathematics, National Tsing Hua University; NA; NA","IEEE Transactions on Software Engineering","18 Sep 2006","1976","SE-2","3","185","193","In this paper, we shall introduce a concept widely used by statisticians, the principal component analysis technique. We shall show that this principal component analysis technique can be used to create new keys from a set of old keys. These new keys are very useful in narrowing down the search domain. We shall also show that the projections on the first principal component direction can be viewed as hashing addresses for the best-match searching problem.","1939-3520","","10.1109/TSE.1976.225946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702364","Baskett and Shustek algorithm;best-match;exact-match;hash coding for best-match searching;multikey searching;principal component analysis;the Friedman;variance","Principal component analysis;Pattern recognition;Information security;Data security;Mathematics;Computer science;Information retrieval;Personnel;Books;Statistics","","Baskett and Shustek algorithm;best-match;exact-match;hash coding for best-match searching;multikey searching;principal component analysis;the Friedman;variance","","35","","16","","18 Sep 2006","","","IEEE","IEEE Journals"
"Comparing uniform and flexible policies for software maintenance and replacement","Y. Tan; V. S. Mookerjee","Bus. Sch., Washington Univ., Seattle, WA, USA; NA","IEEE Transactions on Software Engineering","25 Apr 2005","2005","31","3","238","255","The importance of software maintenance in managing the life-cycle costs of a system cannot be overemphasized. Beyond a point, however, it is better to replace a system rather than maintain it. We derive model and operating policy that reduces the sum of maintenance and replacement costs in the useful life of a software system. The main goal is to compare uniform (occurring at fixed time intervals) versus flexible (occurring at varying, planned time intervals) polices for maintenance and replacement. The model draws from the empirical works of earlier researchers to consider 1) inclusion of user requests for maintenance, 2) scale economies in software maintenance, 3) efficiencies derived from replacing old software technology with new software technology, and 4) the impact of software reuse on replacement and maintenance. Results from our model show that the traditional practice of maintaining or replacing a software system at uniform time intervals may not be optimal. We also find that an increase in software reuse leads to more frequent replacement, but the number of maintenance activities is not significantly impacted.","1939-3520","","10.1109/TSE.2005.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1423995","Index Terms- Software maintenance and replacement;cost models;optimal scheduling.","Software maintenance;Software systems;Application software;Cost function;Environmental economics;Optimal scheduling;Computer industry;Software performance;Personnel;Java","software reusability;software maintenance;software cost estimation","software maintenance;software replacement;software reuse;optimal scheduling;software cost estimation","","31","","57","","25 Apr 2005","","","IEEE","IEEE Journals"
"Observations of Fallibility in Applications of Modern Programming Methodologies","S. L. Gerhart; L. Yelowitz","Department of Computer Science, Duke University; NA","IEEE Transactions on Software Engineering","18 Sep 2006","1976","SE-2","3","195","207","Errors, inconsistencies, or confusing points are noted in a variety of published algorithms, many of which are being used as examples in formulating or teaching principles of such modern programming methodologies as formal specification, systematic construction, and correctness proving. Common properties of these points of contention are abstracted. These properties are then used to pinpoint possible causes of the errors and to formulate general guidelines which might help to avoid further errors. The common characteristic of mathematical rigor and reasoning in these examples is noted, leading to some discussion about fallibility in mathematics, and its relationship to fallibility in these programming methodologies. The overriding goal is to cast a more realistic perspective on the methodologies, particularly with respect to older methodologies, such as testing, and to provide constructive recommendations for their improvement.","1939-3520","","10.1109/TSE.1976.233815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702366","Correctness proofs;fallibility;program correctness;program errors;program specifications;programming methodologies;reliability;testing","Education;Formal specifications;Guidelines;Mathematical programming;Testing;Computer science;Mathematics;Error correction;Frequency;Data structures","","Correctness proofs;fallibility;program correctness;program errors;program specifications;programming methodologies;reliability;testing","","54","","31","","18 Sep 2006","","","IEEE","IEEE Journals"
"A rely and guarantee method for timed CSP: a specification and design of a telephone exchange","A. Kay; J. N. Reed","Comput. Lab., Oxford Univ., UK; Comput. Lab., Oxford Univ., UK","IEEE Transactions on Software Engineering","6 Aug 2002","1993","19","6","625","639","A rely and guarantee method for timed communicating sequential processes (TCPSs), by which the behavior of a component belonging to a composite system is specified in terms of what it guarantees to its neighbors and what it relies on from them, is described. The method is illustrated using an overview of the specification of a plain old telephone service together with part of a design that provably satisfies this specification. The specification and design deal with safety, liveness, and troublesome race conditions.<<ETX>></ETX>","1939-3520","","10.1109/32.232027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=232027","","Telephony;Interconnected systems;Safety;Formal specifications;Switching systems;Protocols;Large-scale systems;Explosions;Europe;Concurrent computing","communicating sequential processes;formal specification;telecommunications computing;telephone exchanges","rely method;telephone exchange;guarantee method;timed communicating sequential processes;specification;telephone service;safety;liveness;troublesome race conditions","","21","","40","","6 Aug 2002","","","IEEE","IEEE Journals"
"Qualified Data Flow Problems","L. H. Holley; B. K. Rosen",IBM Cambridge Scientific Center; NA,"IEEE Transactions on Software Engineering","18 Sep 2006","1981","SE-7","1","60","78","It is known that not aU paths are possible in the run time control flow of many programs. It is also known that data flow analysis cannot restrict attention to exactly those paths that are possible. It is, therefore, usual for analytic methods to consider aU paths. Sharper information can be obtained by considering a recursive set of paths that is large enough to include aUl possible paths, but smaU enough to exclude many of the impossible ones. This paper presents a simple uniform methodology for sharpening data flow information by considering certain recursive path sets of practical importance. Associated with each control flow arc there is a relation on a finite set Q. The paths that qualify to be considered are (essentially) those for which the composition of the relations encountered is nonempty. For example, Q might be the set of all assignments of values to each of several bit variables used by a program to remember some facts about the past and branch accordingly in the future. Given any data-flow problem together with qualifying relations on Q associated with the control flow arcs, we construct a new problem. Considering all paths in the new problem is equivalent to considering only qualifying paths in the old one. Preliminary experiments (with a smaUl set of real programs) indicate that qualified analysis is feasible and substantialy more informative than ordinary analysis.","1939-3520","","10.1109/TSE.1981.234509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702803","Data flow analysis;global variable;interprocedural analysis;label variable;symbolic execution","Testing;Data analysis;Computer languages;Feedback;Assembly;Flowcharts;Constraint optimization;Operating systems;Algorithm design and analysis;Availability","","Data flow analysis;global variable;interprocedural analysis;label variable;symbolic execution","","25","","25","","18 Sep 2006","","","IEEE","IEEE Journals"
"The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment","M. Jorgensen; S. Grimstad","University of Oslo and Simula Research Laboratory, Lysaker; University of Oslo and Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","29 Sep 2011","2011","37","5","695","707","Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects.","1939-3520","","10.1109/TSE.2010.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551161","Cost estimation;software psychology;requirements/specifications.","Estimation;Software;Companies;Laboratories;Programming;Materials;Project management","formal specification;software cost estimation","irrelevant information impact;misleading information impact;software development effort estimates;randomized controlled field experiment;software development projects;original requirement specification;laboratory settings;software psychology","","32","","31","","19 Aug 2010","","","IEEE","IEEE Journals"
"In Memoriam - David Notkin (1953-2013)","B. Nuseibeh",NA,"IEEE Transactions on Software Engineering","23 May 2013","2013","39","6","742","743","David Samuel Notkin, whose technical, educational, and social contributions to computer science and software engineering research made him a major figure in the field, died on 22 April 2013, at his home in Seattle, Washington. He was 58 years old. The cause of his death was cancer. David is best known for his research, with his many graduate students, on software evolution. He asked why software is often so hard and expensive to change, and he worked to reduce the difficulty of software evolution to an essential minimum. This focus came from his belief that the ability to change software - its softness - is where its true but under-realized potential resides. He asked questions such as whether we can identify and close the gap between Brooks' notions of accidental and essential software complexity? How much should rather than does it cost to develop, test, and evolve software? Can we make the cost of change proportionate rather than disproportionate to the apparent complexity of changes to be made? Can we design software analysis methods that realize the best properties of both static and dynamic analysis techniques? Beyond technical contributions, David is widely recognized and admired for his exceptional skill as a research mentor for graduate students and as a powerful and unwavering advocate for improving gender diversity in computer science. A brief biography is given highlighting Notkin's professional achievements.","1939-3520","","10.1109/TSE.2013.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519247","","Obituaries;Notkin, David","","","","","","","","23 May 2013","","","IEEE","IEEE Journals"
