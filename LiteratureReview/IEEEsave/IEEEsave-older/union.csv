,Document Title,Publication Title,Publication Year,Start Page,End Page,DOI,PDF Link,Abstract,Author Keywords
0,An approach to measure software maintenance and support as a value added component of today's business,"20th IEEE International Conference on Software Maintenance, 2004. Proceedings.",2004,503,,10.1109/ICSM.2004.1357851,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357851,"There has been considerable investment by Dofasco in existing manufacturing process software. Also, it has been shown that software must evolve to continue to be useful, and thus enhancement/perfective maintenance work is a common activity in an industrial environment. Since software maintenance can consume as much as 70 % of the lifecycle costs of a software system, there is constant pressure from our internal clients to reduce maintenance costs. This proposal describes an approach of reclassifying maintenance activities to provide a measurement tool for clients and senior management, to see a true picture of efforts spent on ""business support"", ""retain benefits"" and to obtain ""business benefits"".",
0,"Software Systems, Their Engineers and Their Testers","2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)",2018,426,427,10.1109/ICST.2018.00053,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367073,"My overall research goal is to provide meaningful insights, methods and practical tools to help the work of stakeholders during various phases of software development to explore the human factor of IT. My current work mainly considers the analyses of package structure and its relation to the grouping of test cases. With this I aim to help senior developers to find problematic or hard to understand parts of software. During my research I introduced an elaborated test coverage based method to find and classify discrepancies between dynamic and static production and test code grouping. This technique helps both testers and developers to find stray test cases or code elements in Java package structure. In the second to last section I briefly introduce my other two research topics and their possible applications in the field of software testing.",test analyses;test coverage;testing;productivity;visualization;static analyses;dynamic analyses;process metric
1,A System Analysis Study Comparing Reverse Engineered Combinatorial Testing to Expert Judgment,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation",2012,630,635,10.1109/ICST.2012.151,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200163,"The Lockheed Martin F-16 ventral fin redesign-combinatorial test study effort was to demonstrate how Combinatorial Testing (CT) could have been applied to system hardware design flaw analysis. The historic analysis was able to determine a set of combinations, which isolated the problem and tested a solution. However, the original effort was expensive, time consuming, and required highly specialized knowledge from the expert to be effective. The new study was an effort to understand if combinatorial test could be applied to similar situations using the original data but conducted by a less senior person without an expert's knowledgebase. The situation and CT approaches are detailed in this paper. In the study, a series of iterations created combinatorial test cases which could have ""replicated"" the original highly optimized and successful test program, without the expert.",Case-Study;Combinatorial Test;Design problem
2,JAT: A Test Automation Framework for Multi-Agent Systems,2007 IEEE International Conference on Software Maintenance,2007,425,434,10.1109/ICSM.2007.4362655,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362655,"Automated tests have been widely used as a supporting mechanism during software development and maintenance activities. It improves the confidence on software releases as it seeks to uncover regression bugs, and serves as a live documentation which is very useful when evolving systems. Concerning multi agent systems (MASs), some characteristics such as agent autonomy and asynchronous message-based interaction bring a degree of non-determinism which presents new testing challenges. This paper proposes JAT, a framework for building and running MASs test scenarios, which relies on the use of aspect-oriented techniques to monitor the autonomous agents during tests and control the test input of asynchronous test cases. The tool has been developed on top of JADE, a widely used agent platform implemented in Java. We have used JAT on testing 3 different MASs. Our experience shows that JAT can be used to build test scenarios which can achieve high fault-detection effectiveness.",
3,Re-using software architecture in legacy transformation projects,"International Conference on Software Maintenance, 2003. ICSM 2003. Proceedings.",2003,462,,10.1109/ICSM.2003.1235456,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235456,"Summary form only given, as follows. Software engineers sometimes have to take part in the legacy transformation projects, which are characterized by the complete absence of automated migration tools. In such cases, specialists usually aim at reproducing the original system using the new technologies, without adding any new features. It is common knowledge that it makes sense to keep the functionality as close to the original as possible, because in this case on could use the legacy system as an executable set of requirements. We argue that another, less obvious advantage of ""replicating"" the old system is re-use of architectural decisions that built in the original legacy system and usually represent an invaluable treasure, because they reflect an implemented understanding of the application domain.",
4,Digging deep: Software reengineering supported by database reverse engineering of a system with 30+ years of legacy,2009 IEEE International Conference on Software Maintenance,2009,407,410,10.1109/ICSM.2009.5306293,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306293,"This paper describes the industrial experience in performing database reverse engineering on a large scale software reengineering project. The project in question deals with a highly heterogeneous in-house information system (IS) that has grown and evolved in numerous steps over the past three decades. This IS consists of a large number of loosely coupled single purpose systems with a database driven COBOL application at the centre, which has been adopted and enhanced to expose some functionality over the web. The software reengineering effort that provides the context for this paper deals with unifying these components and completely migrating the IS to an up-to-date and homogeneous platform. A database reverse engineering (DRE) process was tailored to suit the project environment consisting of almost 350 tables and 5600 columns. It aims at providing the developers of the software reengineering project with the necessary information about the more than thirty year old legacy databases to successfully perform the data migration. The application of the DRE process resulted in the development of a high-level categorization of the data model, a wiki based redocumentation structure and the essential data-access statistics.",
5,Reverse Engineering PL/SQL Legacy Code: An Experience Report,2014 IEEE International Conference on Software Maintenance and Evolution,2014,553,556,10.1109/ICSME.2014.93,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976137,"The reengineering of legacy code is a tedious endeavor. Automatic transformation of legacy code from an old technology to a new one preserves potential problems in legacy code with respect to obsolete, changed, and new business cases. On the other hand, manual analysis of legacy code without assistance of original developers is time consuming and error-prone. For the purpose of reengineering PL/SQL legacy code in the steel making domain, we developed tool support for the reverse engineering of PL/SQL code into a more abstract and comprehensive representation. This representation then serves as input for stakeholders to manually analyze legacy code, to identify obsolete and missing business cases, and, finally, to support the re-implementation of a new system. In this paper we briefly introduce the tool and present results of reverse engineering PL/SQL legacy code in the steel making domain. We show how stakeholders are supported in analyzing legacy code by means of general-purpose analysis techniques combined with domain-specific representations and conclude with some of the lessons learned.",reverse engineering;program comprehension;source code analysis
6,Automated Refactoring of Legacy Java Software to Enumerated Types,2007 IEEE International Conference on Software Maintenance,2007,224,233,10.1109/ICSM.2007.4362635,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362635,"Java 1.5 introduces several new features that offer significant improvements over older Java technology. In this paper we consider the new enum construct, which provides language support for enumerated types. Prior to Java 1.5, programmers needed to employ various patterns (e.g., the weak enum pattern) to compensate for the absence of enumerated types in Java. Unfortunately, these compensation patterns lack several highly-desirable properties of the enum construct, most notably, type safety. We present a novel fully-automated approach for transforming legacy Java code to use the new enumeration construct. This semantics-preserving approach increases type safety, produces code that is easier to comprehend, removes unnecessary complexity, and eliminates brittleness problems due to separate compilation. At the core of the proposed approach is an interprocedural type inferencing algorithm which tracks the flow of enumerated values. The algorithm was implemented as an Eclipse plug-in and evaluated experimentally on 17 large Java benchmarks. Our results indicate that analysis cost is practical and the algorithm can successfully refactor a substantial number of fields to enumerated types. This work is a significant step towards providing automated tool support for migrating legacy Java software to modern Java technologies.",
7,Software modularization operators,2010 IEEE International Conference on Software Maintenance,2010,1,10,10.1109/ICSM.2010.5609546,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609546,"There exists a number of large business critical software systems written in newer languages such as C and Java that are fast becoming legacy and increasingly difficult to maintain. Unlike older monolithic systems, where modularization primarily involves splitting the monolithic code base into modules, for such newer systems which already have some basic modular structure, code decomposition is only one of the many possible activities. Even though the area of software modularization has received considerable attention over these past years, there are hardly any case studies documented in literature on modularizing large C and Java systems. We still do not fully comprehend the activities experienced developers perform when they have to modularize such newer systems. The goal of this paper is to learn from past software modularization projects and identify common recurring patterns. This paper formalizes 6 such patterns, which we term as modularization operators, that are likely to be the basic building blocks of any software modularization activity. The operators presented in this paper are validated using modularization case studies of open source software systems and a proprietary software system and several observations and insights are presented.",
8,Measuring the progress of projects using the time dependence of code changes,2009 IEEE International Conference on Software Maintenance,2009,329,338,10.1109/ICSM.2009.5306313,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306313,"Tracking the progress of a project is often done through imprecise manually gathered information, like progress reports, or through automatic metrics such as Lines Of Code (LOC). Such metrics are too coarse-grained and too imprecise to capture all facets of a project. In this paper, we mine the code changes in the source code repository and study the concept of time dependence of code changes. Using this concept, we can track the progress of a software project as the progress of a building. We can examine how changes build on each other over time and determine the impact of these changes on the quality of a project. In particular, we study whether new changes are built just-in-time or if they build on older, stable code. Through a case study on two large open source projects (PostgreSQL and FreeBSD), we show that time dependence varies across projects and throughout the lifetime of each project. We also show that there is a high linear correlation between building on new code and the occurrence of bugs.",
9,Toward documentation of program evolution,21st IEEE International Conference on Software Maintenance (ICSM'05),2005,505,514,10.1109/ICSM.2005.92,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510145,"The documentation of a program often falls behind the evolution of the program source files. When this happens it may be attractive to shift the documentation mode from updating the documentation to documenting the evolution of the program. This paper describes tools that support the documentation of program evolution. The tools are refinements of the elucidative programming tools, which in turn are inspired from literate programming tools. The version-aware elucidative programming tools are able to process a set of program source files in different versions together with unversioned documentation files. The paper introduces a set of fine grained program evolution steps, which are supported directly by the documentation tools. The automatic discovery of the fine grained program evolution steps makes up a platform for documenting coarse grained and more high-level program evolution steps. It is concluded that our approach can help revitalize older documentation, and that discovery of the fine grained program evolution steps help the programmer in documenting the evolution of the program.",
10,Checking inside the black box: regression testing based on value spectra differences,"20th IEEE International Conference on Software Maintenance, 2004. Proceedings.",2004,28,37,10.1109/ICSM.2004.1357787,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357787,"Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Traditional regression testing strongly focuses on black-box comparison of program outputs. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavior differences between versions. We present a new class of program spectra, value spectra, which enriches the existing program spectra family. We compare the value spectra of an old version and a new version to detect internal behavior deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavior deviations. We have conducted an experiment on seven C programs to evaluate our approach. The results show that our approach can effectively expose program behavior differences between versions even when their program outputs are the same, and our approach reports deviation roots with high accuracy for most programs.",
11,Automatic test case selection and generation for regression testing of composite service based on extensible BPEL flow graph,2010 IEEE International Conference on Software Maintenance,2010,1,10,10.1109/ICSM.2010.5609541,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609541,"Services are highly reusable, flexible and loosely coupled, which makes the evolution and the maintenance of composite services more complex. Evolution of BPEL composite service covers changes of processes, bindings and interfaces. In this paper, an approach is proposed to select and generate test cases during the evolution of BPEL composite service. The approach identifies the changes by using control-flow analysis technique and comparing the paths in new composite service version and the old one using extensible BPEL flow graph (or XBFG). Message flow is appended to the control flow so that XBFG can describe the behavior of composite service integrally. The binding and predicate constraint information added in XBFG elements can be used in path selection and test case generation. Theory analysis and case study both show that the approach is effective, and test cases coverage rate is high for the changes of processes, bindings and interfaces.",
12,Program analysis and transformation for data-intensive system evolution,2010 IEEE International Conference on Software Maintenance,2010,1,6,10.1109/ICSM.2010.5609724,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609724,"Data-intensive software systems are generally made of a database and a collection of application programs in strong interaction with the former. They constitute critical assets in most enterprises, since they support business activities in all production and management domains. Data-intensive systems form most of the so-called legacy systems: they typically are one or more decades old, they are very large, heterogeneous and highly complex. Many of them significantly resist modifications and change due to the lack of documentation, to the use of aging technologies and to inflexible architectures. Therefore, the evolution of data-intensive systems clearly calls for automated support. This thesis explores the use of automated program analysis and transformation techniques in support to the evolution of the database component of the system. The program analysis techniques aim to ease the database evolution process, by helping the developers to understand the data structures that are to be changed, despite the lack of precise and up-to-date documentation. The objective of the program transformation techniques is to support the adaptation of the application programs to the new database. This adaptation process is studied in the context of two realistic database evolution scenarios, namely database database schema refactoring and database platform migration.",
13,An Empirical Study of Multi-entity Changes in Real Bug Fixes,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),2018,287,298,10.1109/ICSME.2018.00038,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530037,"Prior studies showed that developers applied repeated bug fixes-similar or identical code changes-to multiple locations. According to the observation, researchers built tools to automatically generate candidate patches from the repeated bug-fixing patterns. However, all such research focuses on the recurring change patterns within single methods. We are curious whether there are also repeated bug fixes that change multiple program entities (e.g., classes, methods, and fields); and if so, how we can leverage such recurring change patterns to further help developers fix bugs. In this paper, we present a comprehensive empirical study on multi-entity bug fixes in terms of their frequency, composition, and semantic meanings. Specifically for each bug fix, we first used our approach InterPart to perform static inter-procedural analysis on partial programs (i.e., the old and new versions of changed Java files), and to extract change dependency graphs (CDGs)-graphs that connect multiple changed entities based on their syntactic dependencies. By extracting common subgraphs from the CDGs of different fixes, we identified the recurring change patterns. Our study on Aries, Cassandra, Derby, and Mahout shows that (1) 52-58% of bug fixes involved multi-entity changes; (2) 6 recurring change patterns commonly exist in all projects; and (3) 19-210 entity pairs were repetitively co-changed mainly because the pairs invoked the same methods, accessed the same fields, or contained similar content. These results helped us better understand the gap between the fixes generated by existing automatic program repair (APR) approaches and the real fixes. Our observations will shed light on the follow-up research of automatic program comprehension and modification.",Correlated changes in multiple software entities;change characterization
14,Mainframe Migration Based on Screen Scraping,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),2018,675,684,10.1109/ICSME.2018.00077,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530080,"Companies possess a history and large array of legacy information systems that consume a great part of their IT budget in operations and maintenance. These systems are mission-critical, and they cannot be fully discarded since they retain business rules and provide information that is not available anywhere else. Unfortunately, decades-old legacy systems cannot easily withstand modification. Mainframes specifically conglomerate most of these legacy systems. Although there are some white-box solutions for migrating mainframe systems, such solutions lack systematicity and do not provide mechanisms for verifying business rules preservation. Hence, this paper presents a black-box solution (ignoring the internal structure of COBOL programs) which uses a screen scraping technique for migrating mainframe systems toward JavaFX and relational databases. Together with this solution, this paper provides an automatic verification technique to check if the recreated system reflects all the embedded business logic. This proposal has been designed and developed in the context of an industrial project, in which the solution has already migrated 43,000,000 mainframe screens from four systems. The main implication for researchers and practitioners is that screen scraping has proved to be feasible for migrating mainframe systems in large-scale projects within a manageable time-frame while preserving business.",Screen Scraping;Mainframe;Legacy Information Systems;COBOL;Migration;Verification
15,"Reduce, reuse, recycle, recover: Techniques for improved regression testing",2009 IEEE International Conference on Software Maintenance,2009,5,5,10.1109/ICSM.2009.5306347,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306347,"One of the most expensive activities that occurs as software is developed and maintained is the testing (or retesting) of the software after it has been modified. Studies suggest that a significant portion of development and maintenance costs go to this retesting, which is known as regression testing. Reports estimate that regression testing consumes as much as 80% of the overall testing budget and can consume up to 50% of the cost of software maintenance. Rapidly changing software and computing environments present many challenges for effective and efficient regression testing in practice. Regression testing can be performed after changes are made to the software, such as after nightly or regular builds, before a new version of the software is released, every time the software is saved and compiled, such as in an agile development environment, or before patches, such as security patches, are released. Regardless of the environment or when it is performed, the goals of regression testing are the same: to improve confidence that the changes behave as intended and that they have not adversely affected unchanged parts of the software. Because regression testing is important, but expensive, much research has been performed, both in industry and in academia, to develop techniques to make regression testing more effective and efficient. This research has also produced many tools and systems that have been used for empirical studies that investigate the effectiveness, scalability, and practicality of the techniques. Researchers have developed techniques for addressing a number of issues related to regression testing, and, in this talk, I will discuss them in four areas. First, techniques attempt to reduce the regression testing time by creating effective regression test suites that test the changed part of the software, by identifying test cases in the regression test suite that do not need to be rerun on the changed software, and by identifying and removing obsolete test cases. Second, techniques can reuse test suites created for one version of the software by identifying those test cases that need to be rerun for testing subsequent versions of the software and by computing an effective ordering for running the test cases. Third, techniques can recycle test cases by monitoring executions to gather test inputs that can be used for retest-ing and by creating unit test cases from system test cases. Finally, techniques could recover test cases by identifying, manipulating, and transforming obsolete test cases, by generating new test cases from old ones, and by repairing test cases when the software changes. In this talk, I will overview the research in testing of evolving software, and discuss achievements to date in managing regression testing by reducing, reusing, recycling, and recovering test cases. I will also present the state of the research and the state of the practice in regression testing. Finally, I will discuss the current trends in both academia and industry, the challenges for solving the difficult problems that exist, the promise for testing of evolving software in the future, and the important open challenges for regression testing in the next decade.",
0,Requirements Elicitation with and for Older Adults,IEEE Software,2008,16,17,10.1109/MS.2008.69,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497756,"The study investigated the strengths and weaknesses of existing user-centered design (UCD) methods for eliciting requirements from older adults. A case study in which older people are involved in designing, developing, and evaluating a virtual-garden prototype. Users could potentially employ this device (which has an interface that resembles a real garden) as an empathic information appliance that would provide weather forecasts and help control smart devices and assistive technologies. Creativity-oriented techniques are suggested to designers, developers, and engineers when creating products so that older people are included in the market consideration.",older adults;empathic interfaces;user-centered design
1,Is 40 the New 60? How Popular Media Portrays the Employability of Older Software Developers,IEEE Software,2020,26,31,10.1109/MS.2020.3014178,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157881,"We studied the public discourse around age and software development, focusing on the United States. This work was designed to build awareness among decision makers in software projects to help them anticipate and mitigate challenges that their older employees may face.",
4,Nine Things You Can Do with Old Software,IEEE Software,2008,93,94,10.1109/MS.2008.139,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4602682,"Every new line of code quickly becomes legacy. When that legacy mounts, it forms a significantly massive pile of software, which cannot be ignored. This article discusses what we can do intentionally with such piles, from abandonment to evolution and many things in between.",software architecture;software architecture economics;software development life cycle
5,Open source software ERPs: a new alternative for an old need,IEEE Software,2006,94,97,10.1109/MS.2006.78,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1628946,"Both OSS ERPs and proprietary ERPs involve complex system implementations that usually require modifications to a firm's business processes as well as some adaptation of the ERP' functionality. This mutual matching project usually involves consulting firms. However, the benefits of applying OSS are greater for ERPs than for other kinds of applications, for three main reasons: increased adaptability, decreased reliance on a single supplier, reduced costs",ERP;enterprise resource planning;open source;management information systems
7,Sometimes the Old Ways Are Best,IEEE Software,2008,18,19,10.1109/MS.2008.161,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670705,"Brian Kernighan looks back at what has and has not changed in computing, programming, and tools over the last 30 years. He discusses what we want from our tools and reveals his favorites.",software tool;software development tool;integrated development environment
8,Community Collaboration for ERP Implementation,IEEE Software,2009,48,55,10.1109/MS.2009.171,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5287009,"The paper discusses the enterprise resource planning (ERP).ERP implementation solves business problems by customizing and integrating off-the-shelf enterprise software packages. A successful ERP implementation involves extensive collaboration and communication among the customer, implementation consultancy, and software vendor. Collaboration allows implementation personnel from different organizations to utilize each other's experiences. An example of this is the Web 2.0. A Web 2.0 knowledge repository system can reduce costs, improve quality, and lower the risks of ERP implementations.Through the case study, the key desired features for Web 2.0 knowledge repository system to support ERP implementations were defined. Epics was designed to fulfill these requirements and developed a prototype. Epics can be used by software vendors, ERP consulting firms, and ERP users. Human aspects play a critical role in ERP implementation. ERP implementation quality depends largely on how the implementation personnel's knowledge and past experiences are reused and communicated. Although knowledge reuse, collaboration, and communication are important for all software projects, they're particularly critical in ERP implementation owing to its unique challenges.",Web 2.0;ERP implementation;software engineering;software implementation;packaged software;enterprise resource planning;ERP
12,Are Domain-Specific Models Easier to Maintain Than UML Models?,IEEE Software,2009,19,21,10.1109/MS.2009.87,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076454,"Although domain-specific modeling (DSM) languages have been adopted in industries such as telecommunications and insurance, they haven't yet gained wide acceptance in practice. This is because the claims of increased productivity and ease of understanding haven't yet been verified by independent studies. To address this concern, we examined a DSM language's performance for maintenance tasks. Maintenance in software-intensive systems is critical because software often continuously evolves during development as well as after delivery, to meet users' ever-changing needs. So, maintenance performance significantly impacts software development productivity.Experimental results show that maintenance can be significantly easier and faster with a DSM language than with a general- purpose modeling language.",visual programming;programming techniques;design notations and documentation;design representation;design maintainability;software distribution;software maintenance
13,"Agile Documentation, Anyone?",IEEE Software,2009,11,12,10.1109/MS.2009.167,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5287001,"Software developers are notorious for skimping on design documentation, often eschewing it altogether. This trend has led to claims that it is merely an impediment in the fast-paced and highly pliable world of software development-a useless vestige of old-style engineering that should be eliminated altogether. While recognizing the unique nature of software, the author argues that, because of the complexity of modern software systems and the cryptic nature of current programming languages, good design documentation is not only useful but vital. However, we must seek ways of adapting it to suit the medium as well as the exceptionally dynamic development process.",Documentation;Software engineering;Software maintenance
14,Agile Requirements Engineering Practices: An Empirical Study,IEEE Software,2008,60,67,10.1109/MS.2008.1,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420071,"An analysis of data from 16 software development organizations reveals seven agile RE practices, along with their benefits and challenges. The rapidly changing business environment in which most organizations operate is challenging traditional requirements-engineering (RE) approaches. Software development organizations often must deal with requirements that tend to evolve quickly and become obsolete even before project completion. Rapid changes in competitive threats, stakeholder preferences, development technology, and time-to-market pressures make prespecified requirements inappropriate.",requirements engineering;agile software development
16,Integrate End to End Early and Often,IEEE Software,2013,9,14,10.1109/MS.2013.77,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547611,"This column is all about stories, and this one is as exciting as a paperback whodunit. The details are all included, and I hate to spoil it, but there's a happy ending. The story is about something old--designing and implementing a new system when the old one was really old (two decades!) and something new--using outside research consultants to save the day with a secret sauce. Enjoy!",software architecture;software process;incremental iterative development;integration;architecture-centric engineering;team software process
18,Requirements Engineering's Next Top Model,IEEE Software,2013,24,29,10.1109/MS.2013.129,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648608,"A game-show environment let a panel competitively explore the use of various requirements modeling techniques for specifying a complex problem. Although plain old text and rich pictures emerged as the winners, real-world problems are best modeled using a variety of techniques. The Web extra at http://youtu.be/6vfIwSauj5o is an audio podcast of author Jane Cleland-Huang providing an audio recording of her Requirements column, in which she discusses how a game-show environment at the 2013 European Software Engineering Conference let a panel competitively explore the use of various requirements modeling techniques for specifying a complex problem.",requirements;modeling
19,"Aim, fire [test-first coding]",IEEE Software,2001,87,89,10.1109/52.951502,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=951502,"The author argues that test-first coding is not testing. Test-first coding is not new. It is nearly as old as programming. It is an analysis technique. We decide what we are programming and what we are not programming, and we decide what answers we expect. Test-first is also a design technique.",
22,Guest Editors' Introduction: Dynamically Typed Languages,IEEE Software,2007,28,30,10.1109/MS.2007.140,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302683,"The languages discussed in this special issue have a long history, which is perhaps why some have had several different names over the years. One such language is Lisp, the second-oldest programming language. For years, many somewhat dismissively described languages such as Lisp as ""scripting languages."" Today, we more commonly refer to them as dynamically typed languages, typified by Python and Ruby, and their impact is arguably greater than ever. This issue highlights the practical uses of such languages and shows how they're frequently a vehicle for innovation in the development sphere. This article is part of a special issue on dynamically typed languages.",dynamically typed languages;Lisp;Python;Ruby;Smalltalk;Lua;CLOS
23,Test Optimization Using Software Virtualization,IEEE Software,2006,66,69,10.1109/MS.2006.143,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687863,"Virtualization of operating systems has recently become a hype, although the concept is very old. Virtualization lets us group resources logically and thus abstract from the dependencies that physical implementations create. For operating systems, virtualization lets us reduce resource constraints and expenses for a multitude of interacting hardware and operating systems. This is helpful in scenarios with many such dependencies, such as testing. This article briefly introduces virtualization from a tester's perspective. It summarizes experiences and compares different virtualization approaches: architectural, logical, and physical. Only the test itself remains a necessary reality",software testing;virtual machines;open source
24,Point/Counterpoint,IEEE Software,2007,62,65,10.1109/MS.2007.50,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118652,"The key to successful systems is building what the stakeholders desire. In a way, this point can hardly be gain.said: nobody wants to build what the stakeholders don't desire. But as usual, the devil is in the details. this paper contrasts two examples to illustrate what it can mean to build what stakeholders desire. There's an old saying in software development that the users don't know what they want until you give them what they asked for. Experienced developers often smile ruefully when they hear this, having been told at least once in their careers that a system they had delivered was totally unacceptable when it was exactly (as far as they could tell) what was requested. There's more to delivering good software than following orders. The paper describes what positive role developers can play in discovering and exploring requirements",requirements engineering;software development;agile methods
26,Next-Generation Architects for a Harsh Business World,IEEE Software,2012,9,12,10.1109/MS.2012.37,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6155136,"Walter Ariel Risi proposes some patterns for hiring productive people that remind me of research done in the early days of patterns by Jim Coplien, who found that hyperproductive teams included, in many cases, musicians! In the old days, companies often hired smart people from other disciplines because software engineering or computer science hadn't joined the academic production of certified, degreed contributors. With the current focus now on degrees, I wonder if we've lost something important.",personnel;HR;architects;traditional;nontraditional
27,A tale of three programs,IEEE Software,2017,23,26,10.1109/MS.2017.62,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927921,"A look at the generation of prime numbers offers a cautionary tale about the perils of premature optimization. We can code this in any reasonable programming language, and in quite a few unreasonable ones as well. It can be fun to express the algorithm in Python, C, C++, Scala, or Go, or even in scripting languages such as Tcl or Awk. And, yes, I confess that I've tried most of these, including Awk. Each language offers different features that can simplify the job or make it more interesting. For our current purpose, it'll suffice to stick to just plain old C.In a first attempt, we might come up with the version in Figure 1, which assumes that we provide the value of N in some other way-for example, with a macro directive to the C compiler.",prime numbers;recursion;Sieve of Eratosthenes;software development;software engineering;programming;Robert Morris Sr.
28,Body Sensors: Wireless Access to Physiological Data,IEEE Software,2009,71,73,10.1109/MS.2009.5,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721186,"An area often neglected in software technology is where software meets biology. We'll approach this topic with one of the major interfaces between human beings and information technology: body sensors. These sensors mostly operate as interfaces to relatively small software components attached to or implanted into human bodies, such as pacemakers. They provide bidirectional communication interfaces between a person and a remote information system that provides healthcare services, diagnosis, or upgrades. The authors offer an overview of this rather old yet fast-evolving technology and show how to cope with design constraints.",body sensors;wireless communication;embedded software
29,Git,IEEE Software,2012,100,101,10.1109/MS.2012.61,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188603,"Git is a distributed revision control system available on all mainstream development platforms through a free software license. An important difference of git over its older ancestors is that it elevates the software's revisions to first-class citizens. Developers care deeply about software revisions, and git supports this by giving each developer a complete private copy of the software repository and numerous ways to manage revisions within its context. The ability to associate a local repository with numerous remote ones allows developers and their managers to build a variety of interesting distributed workflows, most of which are impossible to run on a traditional centralized version control system. The local repository also makes git responsive, easy to setup, and able to operate without Internet connectivity. GitHub is a git repository hosting provider that simplifies many repository management tasks through a Web-based user interface while also promoting cooperation in open source projects.",git;distributed version control;GitHub;configuration management
30,Like a River,IEEE Software,2009,10,11,10.1109/MS.2009.74,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814951,"The metaphor of software development as building construction is an old one. Here is a fresh perspective, considering the life cycle of a software-intensive system as a river.",life cycle;software-intensive systems
31,Software's invisible users,IEEE Software,2001,84,88,10.1109/52.922730,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922730,"Software is deterministic. Given a starting state and a fixed series of inputs, software will produce the exact same output every single time those inputs are applied. In fact, many of the technologies we apply during development (reviews, formal proofs, testing, and so forth) rely on this determinism; they would not work otherwise. But if software is deterministic, why do weird things happen? Why is it that we can apply a sequence of inputs and observe a failure and then be unable to reproduce that failure? Why does software work on one machine and fail on another? How is it that you can return from lunch and find your Web browser has crashed when it wasn't being used? The answer is, of course, that modern software processes an enormous number of inputs and only a small percentage of these inputs originate from human users. When testers can't reproduce failures, the reason is that they are only resubmitting the human input without sufficient regard to the operating system return codes and input from runtime libraries. When software fails on a new computer after running successfully on another, the reason can only be that the new system is supplying input that differs in content from the old one. And the browser that crashes when you are at lunch is responding to input from a nonhuman external resource.",
32,Where Have All the Stencils Gone?,IEEE Software,2009,91,92,10.1109/MS.2009.108,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076466,"Requirements analysts rarely use manual stencils nowadays. Technological advances mean that analysts can draw many types of diagrams. In this column, invited columnist Colin Codephirst asks whether this is progress, reflects on the advantages of old-style stencils, and muses on whether these stencils offered some advantages that have been forgotten. He outlines the requirements for a new stencil for analysts in the 21st century.",requirements analysis;methods;stencils
33,Requirements in the medical domain: Experiences and prescriptions,IEEE Software,2002,66,69,10.1109/MS.2002.1049394,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049394,"Research shows that information flow in health care systems is inefficient and prone to error. Data is lost, and physicians must repeat tests and examinations because the results are unavailable at the right place and time. Cases of erroneous medication - resulting from misinterpreted, misunderstood, or missing information - are well known and have caused serious health problems and even death. We strongly believe that through effective use of information technology, we can improve both the quality and efficiency of the health sector's work. Introducing a new system might shift power from old to young, from doctor to nurse, or from medical staff to administration. Few people appreciate loss of power, but even fewer will admit that the loss of power is why they resist the new system. Thus, we must work hard to bring this into the open and help people realize that a new system doesn't have to threaten their positions. Again, knowledge and understanding of a hospital's organizational structure, both official and hidden, is necessary if the system's introduction is to be successful.",
35,Rational Metaprogramming,IEEE Software,2008,78,79,10.1109/MS.2008.15,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420074,"Metaprogramming, using programs to manipulate other programs, is as old as programming. From self-modifying machine code in early computers to expressions involving partially applied functions in modern functional-programming languages, metaprogramming is an essential part of an advanced programmer's arsenal. Everyday metaprogramming involves on-the-fly code production. Representative examples include dynamically generated SQL statements and code created for evaluation at runtime in interpreted languages. Metaprogramming also occurs in programs that spew out HTML or XML. Although we can't classify these markup languages as code, their rich syntactic structure qualifies their generation as metaprogramming. Unfortunately, we commonly produce code on the fly by simply pasting together character strings. This means that it's difficult to verify essential properties of the generated code - such as validity, correctness, and safety - at compile time.",metaprogramming;functional programming;templates;generative programming
36,"Open Collaboration, Data Quality, and COVID-19",IEEE Software,2021,137,141,10.1109/MS.2021.3056642,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9407288,"The flavor of this ""Impact"" department is somewhat different. In a pandemic, everybody has to come together. In April 2020, a call went out in the United Kingdom for groups to informally form and collaborate to study this brutal pathogen in whatever way they could. The five authors of this article, old friends from the geophysical industry with decades of experience in numerical modeling and big data, formed such a group.",
37,Managing requirements for business value,IEEE Software,2002,15,17,10.1109/52.991325,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991325,"Managing requirements for business value? What's there to manage? Aren't they just...there?"" Such perplexity was perfectly understandable in the good old days, when all you had to do was get the requirements from your customer, design your system, and produce one of those awful conformance matrices that demonstrated that you had implemented each and every one of the requirements in your system. Against this dreary backdrop of conformance matrices and the like, it is indeed hard to imagine a role for requirements in the development and execution of business strategy. But, as the song says, the times, they are a changin'. We are now looking outside the traditional boundaries and developing ways of getting maximum business value from our investment in requirements. This paper discusses three ways of realizing this value: managed requirements process, requirements agility, and contractual innovation.",
39,Crowd-Based Ambient Assisted Living to Monitor the Elderly鈥檚 Health Outdoors,IEEE Software,2017,53,57,10.1109/MS.2017.4121217,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8106875,"The SafeNeighborhood approach combines data from multiple sources with collective intelligence. It merges mobile, ambient, and AI technologies with old-fashioned neighborhood ties to create safe outdoor spaces for the elderly. We're exploring AAL techniques in outdoor environments to increase the elderly's independence without them having to interact with technology. Current research in outdoor monitoring relies solely on sensor data.4 Our approach, which we call SafeNeigborhood (SN), crowdsources people in the neighborhood to revise the computer's inferences from contextual and sensor data. So, SN brings the community together to provide a safer environment for the elderly.",human-centered computing;handicapped persons;special needs;healthcare;care for the elderly;ambient assisted living;SafeNeighborhood;software engineering;software development;context-aware and smart healthcare
40,Examining the Rating System Used in Mobile-App Stores,IEEE Software,2016,86,92,10.1109/MS.2015.56,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045413,"Unlike products on Amazon.com, mobile apps are continuously evolving, with new versions rapidly replacing the old ones. Nevertheless, many app stores still use an Amazon-style rating system, which aggregates every rating ever assigned to an app into one store rating. To examine whether the store rating captures the changing user satisfaction levels regarding new app versions, researchers mined the store ratings of more than 10,000 mobile apps in Google Play, every day for a year. Even though many apps' version ratings rose or fell, their store rating was resilient to fluctuations once they had gathered a substantial number of raters. The conclusion is that current store ratings aren't dynamic enough to capture changing user satisfaction levels. This resilience is a major problem that can discourage developers from improving app quality.",mobile apps;Android;Google Play;review systems;rating;software development;software engineering
41,Software process improvement in small organizations: a case study,IEEE Software,2005,68,75,10.1109/MS.2005.162,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524917,"The Capability Maturity Model for software has become a major force in software development process improvement. We looked at the role of process improvement in the context of a small organization. Although the Capability Maturity Model integration is replacing the CMM, we focused on the older CMM. We had to look at more than which CMM key process areas apply to small businesses. Our overall goal was to institute good software development practices, we used the language of the CMM to gain the management's cooperation. Applying these practices is essential to managing growth, yet undertaking this effort without prior experience could impede a small company's innovative nature. This case study's purpose was to investigate the success factors of a software process improvement effort for a small software development organization.",CMM;organizational culture;process improvement;small companies
44,A linguistic-engineering approach to large-scale requirements management,IEEE Software,2005,32,39,10.1109/MS.2005.1,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1377120,"For large software companies, the sheer number of textual requirements presents specific challenges. To find market opportunities, organizations must continuously elicit new requirements and reevaluate old ones as market needs evolve. Developing large, complex software products aimed at broad markets involves identifying and maintaining the link between product requirements and the massive inflow of customers' wishes. Automating this support through linguistic engineering could save considerable time and improve software quality.",requirements engineering;large-scale requirements management;natural language processing;linguistic engineering;requirement relationships;requirements similarity;requirement duplicates;redundancy
45,Is Your Upgrade Worth It? Process Mining Can Tell,IEEE Software,2014,94,100,10.1109/MS.2014.20,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728931,"Software vendors typically release updates and upgrades of their software once or twice a year. Users are then faced with the question of whether the upgrade is worth the price and the trouble. The software industry doesn't provide much evidence that it's worthwhile to upgrade to new releases. The authors propose the use of process mining to prove that upgrading to the next release provides quantifiable benefits to the end user. Process mining capitalizes on the fact that event logs capture information about processes. These events can be used to make processes visible and show the benefits of using a software product's next release. Three groups benefits from this process: end users, software suppliers, and researchers. The authors applied process mining to a medical software product and captured empirical data from 1,400 cases. The data shows that the new version was 11 percent more efficient than the old release.",usability;process mining;product metrics
48,Organizing Security Patterns,IEEE Software,2007,52,60,10.1109/MS.2007.114,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267603,"Early software systems communicated in predefined ways, so they were easy to secure. However, the old ways of ensuring software systems' security and reliability are inadequate on the Internet. Although building secure systems is difficult, retrofitting existing systems to introduce security is even harder. For example, sendmail, the most popular mail transfer agent, has been plagued with security vulnerabilities ever since the Morris worm first exploited it in 1988.",patterns;security;protection
51,How Much of the Software Engineering Old Still Remains New?,IEEE Software,2006,104,104,10.1109/MS.2006.106,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657949,"Oddly enough, many software engineering issues and conflicts that were relevant 20 years ago are still relevant today.",software engineering;programming
53,Perspectives [The changing nature of software evolution; The inevitability of evolution],IEEE Software,2010,26,29,10.1109/MS.2010.103,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5484112,"Summary form only given. Traditionally, software evolution took place after software development put a system in place. However, the pace of change in technology and competition has changed the nature of software evolution to a continuous process, in which there's no neat boundary between development and evolution. Many traditional software development assumptions and practices haven't recognized this changing nature and increasingly find themselves in deep trouble as a result. Minimizing development costs by adopting numerous off-the-shelf products often leads to unaffordable evolution costs as vendors ship new releases and stop supporting the old ones. Assuming that a single form of evolutionary development covers all situations often leads to unrealistic commitments and dead-end systems as situations change.",software evolution;software engineering;software change;development
54,Guest Editor's Introduction: The Promise of Public Software Engineering Data Repositories,IEEE Software,2005,20,22,10.1109/MS.2005.153,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524910,"Scientific discovery related to software is based on a centuries-old paradigm common to all fields of science: setting up hypotheses and testing them through experiments. Repeatedly confirmed hypotheses become models that can describe and predict real-world phenomena. The best-known models in software engineering describe relationships between development processes, cost and schedule, defects, and numerous software ""-ilities"" such as reliability, maintainability, and availability. But, compared to other disciplines, the science of software is relatively new. It's not surprising that most software models have proponents and opponents among software engineers. This introduction to the special issue discusses the power of modeling, the promise of data repositories, and the workshop devoted to this topic.",software engineering methodologies;software engineering validation;model checking;process metrics;software science
55,Novelty in Sameness,IEEE Software,2007,5,7,10.1109/MS.2007.82,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4163015,"Novelty and sameness can conceal each other. Regardless of how things might appear in hindsight, we risk missing the mark if we indiscriminately insist on seeing new proposals and emerging movements simply as reincarnations of old advice, whether to justify or refute them.",incremental development;iterative development;phased development;staged development
56,Designing a World at Your Fingertips: A Look at Mobile User Interfaces,IEEE Software,2012,4,7,10.1109/MS.2012.81,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6265071,"Smart mobile devices have had a huge impact on the world today with new apps being produced at a prodigious rate. How we got to this point has a lot to do with the ease of use that manufacturers and app developers have achieved, which includes aspects such as quick response time, intuitive interfaces, and well-designed functionality. To explore how this came about, IEEE Software Editor-in-Chief Forrest Shull recently spoke with Ben Shneiderman and Ben Bederson, both of whom are former directors of the University of Maryland's Human-Computer Interaction Lab (HCIL), the oldest center in the US focusing on research in HCI.",software;engineering;smart phones;mobile devices;apps;functionality;interfaces;hci;human-computer interaction;lab;HCIL
57,Technology Transfer and the Tech Broker,IEEE Software,2006,5,7,10.1109/MS.2006.142,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687852,"Technology transfer is the process of transferring an idea from its originator to someone who can use it. Typically, this implies the transfer of research developed in academia to industry. While there's no dearth of technologies that academic researchers have available and are anxious to transfer, precious little technology transfer involving academia takes place, at least in the software development community. Why don't we see more tech transfer in our field? What the software community needs is an infrastructure that matches researchers with adopters at a specific point in time and helps them overcome the administrative barriers. One example of a successful technology broker is SERC, a two-decade-old consortium of industrial organizations and universities that facilitates the brokering of technology transfer from academia to industry.",technology transfer
58,Whose bug is it anyway? The battle over handling software flaws,IEEE Software,2004,94,97,10.1109/MS.2004.1270771,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270771,"Attacks exploit vulnerabilities in software code. They come in many forms: logic attacks, Trojan horses, worms and viruses, and variants of each. They serve a host of purposes: corporate espionage, white-collar crime, social ""hacktivism,"" terrorism, and notoriety. Greater connectivity, more complex software, and the persistence of older protocols ensure growing vulnerability. End users lose time and money when networks go down. Software vendors lose face and market share. Security researchers struggle to keep pace with the bugs to keep businesses operating safely. The only people with no complaints are the hackers, who reverse-engineer patches released by vendors to exploit the holes. It's enough to make you nostalgic for the old days of the Nimba and Code Red viruses, when attacks came six months after vendors released patches. Blaster attacks began three weeks after release. Security experts anticipate so-called ""zero day"" vulnerabilities, in which attacks precede patches. Although marathon patching sessions have become the norm for harried IT administrators, even top-of-the-line patch management can't keep up with malicious code's growing sophistication.",
59,"Mom, Where Are the Girls?",IEEE Software,2021,3,6,10.1109/MS.2020.3044410,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354391,"During the fall semester of 2005, I was working hastily on the finishing touches of my Ph.D. dissertation at Carnegie Mellon University. That semester, I also was the teaching assistant for the Methods of Software Development graduate course taught by Dr. Mary Shaw and Dr. Jim Herbsleb. It was a busy time, with the challenges of finishing graduate school; getting ready for a new job; fulfilling responsibilities such as grading and helping students; and parenting my then three-and-a-half-year-old daughter. Methods of Software Development was a demanding course with a lot of reading and reflection assignments. Students took abundant advantage of the office hours. Those meetings always went better if I remembered the students' names, but with that was all going on, my brain did not always comply, so I had a hack. I had printed all of their photos and hung them right above my desk.",
0,Applicable Micropatches and Where to Find Them: Finding and Applying New Security Hot Fixes to Old Software,"2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",2021,394,405,10.1109/ICST49551.2021.00051,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438561,"With the complexity and interdependency of modern software sharply rising, the impact of security vulnerabilities and thus the value of broadly available patches has increased drastically. Despite this, it is unclear if the current landscape supports the same level of patch discoverability as that of vulnerabilities - raising questions about whether patches are simply scare or if they are just hard to find (i.e., are there a lot of ""secret patches"" [1]), and, equally important, what kind of patches are they (e.g., are they micropatchable). We seek to assess the current state of patching by analyzing patches for a four-month period of recent Common Vulnerabilities and Exposures (CVEs). At first glance, the state of patching seems abysmal - only one-fourth of CVEs have a labelled patch on the National Vulnerability Database (NVD). However, by searching for indicators on other popular security trackers (e.g., Debian's), we were able to find a lot more ""secret patches"", but the ratio of patched CVEs plateaued around fifty percent.Examining the discovered patches, we noticed that many were version updates, and less than one-tenth had machine accessible source-code micropatches. Using a custom tool that leverages contemporary version control, we were able to test the feasibility of automatically applying these micropatches to older versions of the software and found that approximately two-thirds of the patches can be applied to at least one old version. The failure cases were mostly due to lax practices pertaining to security fixes and general software development (e.g., releasing the fix along with other extraneous features). Reflecting on our investigations, we surmise that between existence, discoverability, and versatility of security patches, existence and discoverability are the bigger problems. As to why this is the case, we find that the answer may lie in the perverse incentive structures of the industry. We conclude with possible remediations and hope that our work at least raises public awareness of the current state of patching and encourages future work to improve the situation.",micropatching;patch discovery;patch testing
1,X10X: Model Checking a New Programming Language with an "Old" Model Checker,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation",2012,11,20,10.1109/ICST.2012.81,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200092,"Parallel and distributed computing is becoming a norm with the advent of multi-core, networked, and cloud computing platforms. New programming languages are emerging for these platforms, e.g., the X10 language from IBM. While these languages explicitly support concurrent programming, they cannot eliminate all concurrency related bugs, which are usually hard to find. Finding such bugs is easier using specialized, language-aware model-checking tools. However, such tools are highly complex and developing them from scratch requires large effort. We describe our experience in developing a model-checking tool for a new language, X10, by systematically adapting an existing tool, the JPF model checker for Java. X10 programs can be compiled to Java, but unfortunately checking X10 programs directly with the unmodified JPF and X10 runtime can miss some behaviors and scales very poorly. We present four sets of techniques that can be employed to make checking a new language with an "" old"" model checker practical: (1) modify the model checker, (2) modify the language runtime, (3) extend the language compiler, and (4) develop a new static analysis. We instantiated each technique to enable checking X10 programs with JPF. We evaluated our new techniques on over 100 X10 example programs and found a substantial speedup.",
2,Canopus: A Domain-Specific Language for Modeling Performance Testing,"2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)",2016,157,167,10.1109/ICST.2016.13,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515468,"Despite all the efforts to reduce the cost of the testing phase in software development, it is still one of the most expensive phases. In order to continue to minimize those costs, in this paper, we propose a Domain-Specific Language (DSL), built on top of MetaEdit+ language workbench, to model performance testing for web applications. Our DSL, called Canopus, was developed in the context of a collaboration<sup>1</sup> between our university and a Technology Development Laboratory (TDL) from an Information Technology (IT) company. We present, in this paper, the Canopus metamodels, its domain analysis, a process that integrates Canopus to Model-Based Performance Testing, and applied it to an industrial case study.",performance testing;domain-specific language;domain-specific modeling;model-based testing
3,A Framework to Support Research in and Encourage Industrial Adoption of Regression Testing Techniques,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation",2012,907,908,10.1109/ICST.2012.194,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200107,"When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.",open-source framework;regression testing
4,Automated Behavioral Regression Testing,"2010 Third International Conference on Software Testing, Verification and Validation",2010,137,146,10.1109/ICST.2010.64,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477091,"When a program is modified during software evolution, developers typically run the new version of the program against its existing test suite to validate that the changes made on the program did not introduce unintended side effects (i.e., regression faults). This kind of regression testing can be effective in identifying some regression faults, but it is limited by the quality of the existing test suite. Due to the cost of testing, developers build test suites by finding acceptable tradeoffs between cost and thoroughness of the tests. As a result, these test suites tend to exercise only a small subset of the program's functionality and may be inadequate for testing the changes in a program. To address this issue, we propose a novel approach called Behavioral Regression Testing (BERT). Given two versions of a program, BERT identifies behavioral differences between the two versions through dynamical analysis, in three steps. First, it generates a large number of test inputs that focus on the changed parts of the code. Second, it runs the generated test inputs on the old and new versions of the code and identifies differences in the tests' behavior. Third, it analyzes the identified differences and presents them to the developers. By focusing on a subset of the code and leveraging differential behavior, BERT can provide developers with more (and more detailed) information than traditional regression testing techniques. To evaluate BERT, we implemented it as a plug-in for Eclipse, a popular Integrated Development Environment, and used the plug-in to perform a preliminary study on two programs. The results of our study are promising, in that BERT was able to identify true regression faults in the programs.",Regression testing;software evolution;dynamic analysis
5,Tedsuto: A General Framework for Testing Dynamic Software Updates,"2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)",2016,278,287,10.1109/ICST.2016.27,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515479,"Dynamic software updating (DSU) is a technique for patching running programs, to fix bugs or add new features. DSU avoids the downtime of stop-and-restart updates, but creates new risks -- an incorrect or ill-timed dynamic update could result in a crash or misbehavior, defeating the whole purpose of DSU. To reduce such risks, dynamic updates should be carefully tested before they are deployed. This paper presents Tedsuto, a general testing framework for DSU, along with a concrete implementation of it for Rubah, a state-of-the-art Java-based DSU system. Tedsuto uses system-level tests developed for the old and new versions of the updateable software, and systematically tests whether a dynamic update might result in a test failure. Very often this process is fully automated, while in some cases (e.g., to test new-version functionality) some manual annotations are required. To evaluate Tedsuto's efficacy, we applied it to dynamic updates previously developed (and tested in an ad hoc manner) for the H2 SQL database server and the CrossFTP server -- two real-world, multithreaded systems. We used three large test suites, totalling 446 tests, and we found a variety of update-related bugs quickly, and at low cost.",
6,Extension-Aware Automated Testing Based on Imperative Predicates,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",2019,25,36,10.1109/ICST.2019.00013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730211,"Bounded exhaustive testing (BET) techniques have been shown to be effective for detecting faults in software. BET techniques based on imperative predicates, enumerate all test inputs up to the given bounds such that each test input satisfies the properties encoded by the predicate. The search space is bounded by the user, who specifies the number of objects of each type and the list of values for each field of each type. To optimize the search, existing techniques detect isomorphic instances and record accessed fields during the execution of a predicate. However, these optimizations are extension-unaware, i.e., they do not speed up the search when the predicate is modified, say due to a fix or additional properties. We present a technique, named iGen, that speeds up test generation when imperative predicates are extended. iGen memoizes intermediate results of a test generation and reuses the results in a future search - even when the new search space differs from the old space. We integrated our technique in two BET tools (one for Java and one for Python) and evaluated these implementations with several data structure pairs, including two pairs from the Standard Java Library. Our results show that iGen speeds up test generation by up to 46.59x for the Java tool and up to 49.47x for the Python tool. Additionally, we show that the speedup obtained by iGen increases for larger test instances.",Bounded exhaustive testing;Imperative predicates;Korat;Alloy
7,Language-Agnostic Generation of Compilable Test Programs,"2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)",2020,39,50,10.1109/ICST46399.2020.00015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159098,"Testing is an integral part of the development of compilers and other language processors. To automatically create large sets of test programs, random program generators, or fuzzers, have emerged. Unfortunately, existing approaches are either language-specific (and thus require a rewrite for each language) or may generate programs that violate rules of the respective programming language (which limits their usefulness). This work introduces *Smith, a language-agnostic framework for the generation of valid, compilable test programs. It takes as input an abstract attribute grammar that specifies the syntactic and semantic rules of a programming language. It then creates test programs that satisfy all these rules. By aggressively pruning the search space and keeping the construction as local as possible, *Smith can generate huge, complex test programs in short time. We present four case studies covering four real-world programming languages (C, Lua, SQL, and SMT-LIB 2) to show that *Smith is both efficient and effective, while being flexible enough to support programming languages that differ considerably. We found bugs in all four case studies. For example, *Smith detected 165 different crashes in older versions of GCC and LLVM.",fuzz testing;compilers;attribute grammars
0,Collecting and analyzing Web-based project metrics,IEEE Software,2002,52,58,10.1109/52.976941,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976941,Metrics generated from real project data provide a valuable tool for evaluating and guiding performance. The authors describe their experience developing a Web-based project metrics system and encouraging its adoption. They also examine how such a system might encourage the adoption of metrics within a company.,
1,Evaluating Software Project Managers: A Multidimensional Perspective,IEEE Software,2017,104,108,10.1109/MS.2017.4121223,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8106867,"Qualified, motivated project managers are key contributors to software organizations. Experts have identified a capable project manager as the most important factor in a software project's success. Project managers' value to software projects, software engineers, and their companies is unquestionable. Thus, getting the most out of those managers is critical. The literature about knowledge workers' psychological profiles show that the best motivator is feedback about how well they've done. This is consistent with our experience in software project management and consulting. Frequent, detailed feedback can also be a positive learning experience and an opportunity to improve skills. Feedback and recognition require the evaluation of professionals, who must accept responsibility for their work if they're going to consider assessments as an opportunity instead of a burden. However, the criteria needed to evaluate professionals aren't obvious. Unfortunately, the literature provides little empirical data about evaluating software project managers. At best, the literature refers to assessing managers in terms of whether a project meets or exceeds its time and cost requirements. Our combined 50 years' experience in software project management (see the 鈥淥ur Experience in Software Project Management鈥 sidebar) has revealed some best practices for evaluating software project managers.",software project managers;evaluations;multidimensional;senior managers;software engineers;clients;balanced scorecard;software engineering;software development;Voice of Evidence
2,Asimov's Laws of Robotics Applied to Software,IEEE Software,2007,112,112,10.1109/MS.2007.100,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267613,"In 1940, science fiction writer Isaac Asi- mov formulated the now-famous Three Laws of Robotics, which constrain robots to serve their human masters. These laws focused on the physical well-being of robots and humans. But they can also be applied to software in a more abstract manner, by focusing on human presence in cyberspace. In this vein, I suggest the following laws of software. While they are largely a restatement of known principles, uniting them in this way casts a new light on the contract between humanity and technology.",Asimov's Laws of Robotics;software requirements
6,Critical Requirements Engineering in Practice,IEEE Software,2020,17,24,10.1109/MS.2019.2944784,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854246,"Software can raise issues about ethics, power, politics, and values. We show how Critical Systems Heuristics can be used to structure explorations of early requirements and provide a framework for developing a reflective understanding for justifying the project and system scope.",
7,Integrating usability techniques into software development,IEEE Software,2001,46,53,10.1109/52.903166,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=903166,"Now merged into Siemens Medical Solutions Health Services, our company, formerly called Shared Medical Systems, creates clinical, financial, and administrative software for the healthcare industry. Like other medium to large companies, SMS had reached a scale and maturity level that required the development process to be documented, predictable, repeatable, measurable, and usable by the development groups. After much study and consideration, senior management committed to implementing a universal OO development methodology. Senior management recognized the need to improve customer satisfaction, which had always been high but needed to he better in an increasingly competitive market. Management saw the introduction of usability practices as a prime means to achieve this objective. So they began to place greater emphasis on usability-even to the point of building and staffing dual state-of-the-art usability labs. Our goal throughout the projects we describe was to combine the best OO analysis and design practices and usability techniques to create a powerful, unified way to develop software.",
8,Managing Slowdown in Improvement Projects,IEEE Software,2008,84,89,10.1109/MS.2008.151,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670719,"Process improvement projects depend on the active participation of managers and senior engineers. At the telecom company Ericsson, change agents were frustrated that requests to participate in improvement projects often precipitated behavior from colleagues that hampered progress. However, rather than accepting slowdowns, the change agents started to ask why managers and senior engineers reacted this way even though Ericsson is highly committed to process improvement. In this article, we show how to analyze slowdown behavior, increase commitment and reinforce progress.",process improvement;progress;commitment;effort
9,Catching the brass ring [software engineering professional],IEEE Software,2004,12,14,10.1109/MS.2004.1293066,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1293066,"Eventhough most senior executives in the larger software firms boast engineering, marketing, legal, and business school backgrounds, most engineering organizations aren't led by the software people. We discuss about the factors that govern such selections and what software engineering professionals can do about it. We also discuss about why the software professionals often aren't tapped for top corporate leadership positions and about their role in the business organisations.",
10,Lessons Learned from Leading Workshops about Geographically Distributed Agile Teams,IEEE Software,2013,7,10,10.1109/MS.2013.33,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6470584,"We've been teaching workshops about geographically distributed agile teams together and separately since March 2011. In that time, we've led workshops in New Zealand, Australia, Israel, Germany, and multiple times in the US, Saudi Arabia, and Canada. But it doesn't matter where we hold these workshops or where the project leaders or the company senior managers are located, we hear many of the same stories. The players vary, the context is often different, but underneath it all, we've seen a set of common mistakes and success factors. In this article, we bring these common aspects together into a set of lessons learned, with some concrete advice on how to avoid the mistakes and leverage and amplify the successes.",geographically distributed teams;project management;software teams
11,Practicing What We Preach,IEEE Software,2014,88,92,10.1109/MS.2014.10,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750452,"The rhetorical question ""do we practice what we preach?"" still seems to be relevant, even a decade after it appeared on the requirements engineering research landscape. New perspectives from various seasoned professionals from India address the question yet again. The Web extra at http://youtu.be/QtJcaibyetw is an audio podcast of Requirements column editor Jane Cleland-Huang speaking with Tony Gorschek, a professor of software engineering at the Blekinge Institute of Technology with more than 10 years of experience as a CTO, senior executive consultant, engineer, chief architect, and product manager, about technology transfer in the requirements engineering field.",requirements;requirements engineering;fieldwork;requirements practitioners;India
12,Reducing Internet-based intrusions: Effective security patch management,IEEE Software,2003,50,57,10.1109/MS.2003.1159029,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1159029,"The Software Productivity Consortium (the Consortium) has been investigating methods for improving and measuring four essential defenses against Internet-based threats: security patch management, system and application hardening, network reconnaissance and enumeration, and tools against malicious software. These defenses increasingly are critical to an organization's information security posture and should be implemented in an effective, systematic, and repeatable fashion. Senior-level managers or executives should review process measurement data regularly to ensure that these defenses are being performed properly and to provide an objective basis for organizational improvement. This article focuses on lessons learned implementing improvements in the first of these defenses, security patch management, and is derived largely from pilot projects conducted in collaboration with Consortium members. The need for improved security patch management figured prominently in the recent draft cyber security strategy issued by the White House. The practices examined in this article can assist organizations in substantially reducing the risk from Internet-based compromises.",
13,Jeremy Miller on Waterfall Versus Agile,IEEE Software,2020,107,C3,10.1109/MS.2020.2987493,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121615,"Jeremy Miller is a senior software architect at Calavista Software. He is involved in open source .NET development as the author of StructureMap and Storyteller and as the lead developer of Marten. In Episode 401 of 鈥淪oftware Engineering Radio,鈥 host Jeff Doolittle spoke with Miller about Waterfall versus Agile, Extreme Programming, pair programming, specialization and self-contained teams, the emergence of Scrum, YAGNI, Agile teams, reversibility, nonfunctional requirements, integration testing, and abstraction and encapsulation. We provide summary excerpts in this column; to hear the full interview, visit http://www.se-radio.net or access our archives via RSS at http://feeds.feedburner.com/se-radio.",
0,"A Systematic Literature Review on Bad Smells鈥5 W's: Which, When, What, Who, Where",IEEE Transactions on Software Engineering,2021,17,66,10.1109/TSE.2018.2880977,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8532309,"Bad smells are sub-optimal code structures that may represent problems needing attention. We conduct an extensive literature review on bad smells relying on a large body of knowledge from 1990 to 2017. We show that some smells are much more studied in the literature than others, and also that some of them are intrinsically inter-related (which). We give a perspective on how the research has been driven across time (when). In particular, while the interest in duplicated code emerged before the reference publications by Fowler and Beck and by Brown et al., other types of bad smells only started to be studied after these seminal publications, with an increasing trend in the last decade. We analyzed aims, findings, and respective experimental settings, and observed that the variability of these elements may be responsible for some apparently contradictory findings on bad smells (what). Moreover, we could observe that, in general, papers tend to study different types of smells at once. However, only a small percentage of those papers actually investigate possible relations between the respective smells (co-studies), i.e., each smell tends to be studied in isolation. Despite of a few relations between some types of bad smells have been investigated, there are other possible relations for further investigation. We also report that authors have different levels of interest in the subject, some of them publishing sporadically and others continuously (who). We observed that scientific connections are ruled by a large 鈥渟mall world鈥 connected graph among researchers and several small disconnected graphs. We also found that the communities studying duplicated code and other types of bad smells are largely separated. Finally, we observed that some venues are more likely to disseminate knowledge on Duplicate Code (which often is listed as a conference topic on its own), while others have a more balanced distribution among other smells (where). Finally, we provide a discussion on future directions for bad smell research.",Software maintenance;reengineering;bad smell
5,Predicting fault incidence using software change history,IEEE Transactions on Software Engineering,2000,653,661,10.1109/32.859533,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859533,"This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight.",
8,Whence to Learn? Transferring Knowledge in Configurable Systems using BEETLE,IEEE Transactions on Software Engineering,2020,1,1,10.1109/TSE.2020.2983927,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050841,"As software systems grow in complexity and the space of possible configurations increases exponentially, finding the near-optimal configuration of a software system becomes challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, collecting enough sample configurations can be very expensive since each such sample requires configuring, compiling, and executing the entire system using a complex test suite. When learning on new data is too expensive, it is possible to use Transfer Learning to 鈥渢ransfer鈥 old lessons to the new context. Traditional transfer learning has a number of challenges, specifically, (a) learning from excessive data takes excessive time, and (b) the performance of the models built via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer learning framework called BEETLE, which is a 鈥渂ellwether鈥-based transfer learner that focuses on identifying and learning from the most relevant source from amongst the old data. This paper evaluates BEETLE with 57 different software configuration problems based on five software systems (a video encoder, an SAT solver, a SQL database, a high-performance C-compiler, and a streaming data analytics tool). In each of these cases, BEETLE found configurations that are as good as or better than those found by other state-of-the-art transfer learners while requiring only a fraction 1/7th of the measurements needed by those other methods. Based on these results, we say that BEETLE is a new high-water mark in optimally configuring software.",Performance Optimization;SBSE;Transfer Learning;Bellwether
10,On the Semantics of Associations and Association Ends in UML,IEEE Transactions on Software Engineering,2007,238,251,10.1109/TSE.2007.37,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123326,"Association is one of the key concepts in UML that is intensively used in conceptual modeling. Unfortunately, in spite of the fact that this concept is very old and is inherited from other successful modeling techniques, a fully unambiguous understanding of it, especially in correlation with other newer concepts connected with association ends, such as uniqueness, still does not exist. This paper describes a problem with one widely assumed interpretation of the uniqueness of association ends, the restrictive interpretation, and proposes an alternative, the intentional interpretation. Instead of restricting the association from having duplicate links, uniqueness of an association end in the intentional interpretation modifies the way in which the association end maps an object of the opposite class to a collection of objects of the class at that association end. If the association end is unique, the collection is a set obtained by projecting the collection of all linked objects. In that sense, the uniqueness of an association end modifies the view to the objects at that end, but does not constrain the underlying object structure. This paper demonstrates how the intentional interpretation improves expressiveness of the modeling language and has some other interesting advantages. Finally, this paper gives a completely formal definition of the concepts of association and association ends, along with the related notions of uniqueness, ordering, and multiplicity. The semantics of the UML actions on associations are also defined formally",Object-oriented modeling;Unified Modeling Language (UML);association;association end;formal semantics;conceptual modeling;model-driven development.
12,Coverage Prediction for Accelerating Compiler Testing,IEEE Transactions on Software Engineering,2021,261,278,10.1109/TSE.2018.2889771,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588375,"Compilers are one of the most fundamental software systems. Compiler testing is important for assuring the quality of compilers. Due to the crucial role of compilers, they have to be well tested. Therefore, automated compiler testing techniques (those based on randomly generated programs) tend to run a large number of test programs (which are test inputs of compilers). The cost for compilation and execution for these test programs is significant. These techniques can take a long period of testing time to detect a relatively small number of compiler bugs. That may cause many practical problems, e.g., bringing a lot of costs including time costs and financial costs, and delaying the development/release cycle. Recently, some approaches have been proposed to accelerate compiler testing by executing test programs that are more likely to trigger compiler bugs earlier according to some criteria. However, these approaches ignore an important aspect in compiler testing: different test programs may have similar test capabilities (i.e., testing similar functionalities of a compiler, even detecting the same compiler bug), which may largely discount their acceleration effectiveness if the test programs with similar test capabilities are executed all the time. Test coverage is a proper approximation to help distinguish them, but collecting coverage dynamically is infeasible in compiler testing since most test programs are generated on the fly by automatic test-generation tools like Csmith. In this paper, we propose the first method to predict test coverage statically for compilers, and then propose to prioritize test programs by clustering them according to the predicted coverage information. The novel approach to accelerating compiler testing through coverage prediction is called COP (short for COverage Prediction). Our evaluation on GCC and LLVM demonstrates that COP significantly accelerates compiler testing, achieving an average of 51.01 percent speedup in test execution time on an existing dataset including three old release versions of the compilers and achieving an average of 68.74 percent speedup on a new dataset including 12 latest release versions. Moreover, COP outperforms the state-of-the-art acceleration approach significantly by improving <inline-formula><tex-math notation=""LaTeX"">$17.16\%\sim 82.51\%$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mrow><mml:mn>17</mml:mn><mml:mo>.</mml:mo><mml:mn>16</mml:mn><mml:mo>%</mml:mo><mml:mo>鈭</mml:mo><mml:mn>82</mml:mn><mml:mo>.</mml:mo><mml:mn>51</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""chen-ieq1-2889771.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> speedups in different settings on average.",Compiler testing;test prioritization;machine learning
13,Dynamic Update of Discrete Event Controllers,IEEE Transactions on Software Engineering,2020,1220,1240,10.1109/TSE.2018.2876843,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500345,"Discrete event controllers are at the heart of many software systems that require continuous operation. Changing these controllers at runtime to cope with changes in its execution environment or system requirements change is a challenging open problem. In this paper we address the problem of dynamic update of controllers in reactive systems. We present a general approach to specifying correctness criteria for dynamic update and a technique for automatically computing a controller that handles the transition from the old to the new specification, assuring that the system will reach a state in which such a transition can correctly occur and in which the underlying system architecture can reconfigure. Our solution uses discrete event controller synthesis to automatically build a controller that guarantees both progress towards update and safe update.",Controller synthesis;dynamic update;adaptive systems
14,An Empirical Study of Dependency Downgrades in the npm Ecosystem,IEEE Transactions on Software Engineering,2019,1,1,10.1109/TSE.2019.2952130,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894401,"In a software ecosystem, a dependency relationship enables a client package to reuse a certain version of a provider package. Packages in a software ecosystem often release versions containing bug fixes, new functionalities, and security enhancements. Hence, updating the provider version is an important maintenance task for client packages. Despite the number of investigations about dependency updates, there is a lack of studies about dependency downgrades in software ecosystems. A downgrade indicates that the adopted version of a provider package is not suitable to the client package at a certain moment. In this paper, we investigate downgrades in the npm ecosystem. We address three research questions. In our first RQ, we provide a list of the reasons behind the occurrence of downgrades. Two categories of downgrades according to their rationale: reactive and preventive. The reasons behind reactive downgrades are defects in a specific version of a provider, unexpected feature changes in a provider, and incompatibilities. In turn, preventive downgrades are an attempt to avoid issues in future releases. In our second RQ, we investigate how the versioning of dependencies is modified when a downgrade occurs. We observed that 49% of the downgrades are performed by replacing a range of acceptable versions of a provider by a specific old version. Also, 48% of the downgrades reduce the provider version by a minor level (e.g., from 2.1.0 to 2.0.0). In our third RQ we observed that 50% of the downgrades are performed at a rate that is four times as slow as the median time-between-releases of their associated client packages. We also observed that downgrades that follow an explicit update of a provider package occur 9 times faster than downgrades that follow an implicit update. Explicit updates occur when the provider is updated by means of an explicit change to the versioning specification (i.e., the string used by client packages to define the provider version that they are willing to adopt). We conjecture that, due to the controlled nature of explicit updates, it is easier for client packages to identify the provider that is associated with the problem that motivated the downgrade.",downgrades;dependency management;npm;software ecosystems
15,Dynamic Software Updating Using a Relaxed Consistency Model,IEEE Transactions on Software Engineering,2011,679,694,10.1109/TSE.2010.79,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551162,"Software is inevitably subject to changes. There are patches and upgrades that close vulnerabilities, fix bugs, and evolve software with new features. Unfortunately, most traditional dynamic software updating approaches suffer some level of limitations; few of them can update multithreaded applications when involving data structure changes, while some of them lose binary compatibility or incur nonnegligible performance overhead. This paper presents POLUS, a software maintenance tool capable of iteratively evolving running unmodified multithreaded software into newer versions, yet with very low performance overhead. The main idea in POLUS is a relaxed consistency model that permits the concurrent activity of the old and new code. POLUS borrows the idea of cache-coherence protocol in computer architecture and uses a 鈥漛idirectional write-through鈥 synchronization protocol to ensure system consistency. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd, and Apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead on the three applications-a less than 1 percent performance degradation (but 5 percent for one case). The time to apply an update is also minimal.",Maintainability;reliability;runtime environments.
17,A Screening Test for Disclosed Vulnerabilities in FOSS Components,IEEE Transactions on Software Engineering,2019,945,966,10.1109/TSE.2018.2816033,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8316943,"Free and Open Source Software (FOSS) components are ubiquitous in both proprietary and open source applications. Each time a vulnerability is disclosed in a FOSS component, a software vendor using this component in an application must decide whether to update the FOSS component, patch the application itself, or just do nothing as the vulnerability is not applicable to the older version of the FOSS component used. This is particularly challenging for enterprise software vendors that consume thousands of FOSS components and offer more than a decade of support and security fixes for their applications. Moreover, customers expect vendors to react quickly on disclosed vulnerabilities-in case of widely discussed vulnerabilities such as Heartbleed, within hours. To address this challenge, we propose a screening test: a novel, automatic method based on thin slicing, for estimating quickly whether a given vulnerability is present in a consumed FOSS component by looking across its entire repository. We show that our screening test scales to large open source projects (e.g., Apache Tomcat, Spring Framework, Jenkins) that are routinely used by large software vendors, scanning thousands of commits and hundred thousands lines of code in a matter of minutes. Further, we provide insights on the empirical probability that, on the above mentioned projects, a potentially vulnerable component might not actually be vulnerable after all.",Security maintenance;security vulnerabilities;patch management;free and open source software
19,Checking inside the black box: regression testing by comparing value spectra,IEEE Transactions on Software Engineering,2005,869,883,10.1109/TSE.2005.107,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542068,"Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Black-box program outputs have been used to characterize program behaviors and they are compared over program versions in traditional regression testing. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavioral differences between versions. In this paper, we present a new class of program spectra, value spectra, that enriches the existing program spectra family. We compare the value spectra of a program's old version and new version to detect internal behavioral deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavioral deviations. We also use path spectra (previously proposed program spectra) to approximate the program states in value spectra. We then similarly compare path spectra to detect behavioral deviations and locate deviation roots in the new version. We have conducted an experiment on eight C programs to evaluate our spectra-comparison approach. The results show that both value-spectra-comparison and path-spectra-comparison approaches can effectively expose program behavioral differences between program versions even when their program outputs are the same, and our value-spectra-comparison approach reports deviation roots with high accuracy for most programs.",Index Terms- Program spectra;regression testing;software testing;empirical studies;software maintenance.
20,Automating Live Update for Generic Server Programs,IEEE Transactions on Software Engineering,2017,207,225,10.1109/TSE.2016.2584066,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497481,"The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents <italic>Mutable Checkpoint-Restart</italic> (<italic>MCR</italic>), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (<italic>Apache httpd</italic>, <italic>nginx</italic>, <italic>OpenSSH</italic> and <italic>vsftpd</italic>) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9<inline-formula><tex-math notation=""LaTeX"">$\times$ </tex-math><alternatives><inline-graphic xlink:href=""giuffrida-ieq1-2584066.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> on average, without optimizations).",Live update;DSU;checkpoint-restart;quiescence detection;record-replay;garbage collection
24,Comparing uniform and flexible policies for software maintenance and replacement,IEEE Transactions on Software Engineering,2005,238,255,10.1109/TSE.2005.30,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1423995,"The importance of software maintenance in managing the life-cycle costs of a system cannot be overemphasized. Beyond a point, however, it is better to replace a system rather than maintain it. We derive model and operating policy that reduces the sum of maintenance and replacement costs in the useful life of a software system. The main goal is to compare uniform (occurring at fixed time intervals) versus flexible (occurring at varying, planned time intervals) polices for maintenance and replacement. The model draws from the empirical works of earlier researchers to consider 1) inclusion of user requests for maintenance, 2) scale economies in software maintenance, 3) efficiencies derived from replacing old software technology with new software technology, and 4) the impact of software reuse on replacement and maintenance. Results from our model show that the traditional practice of maintaining or replacing a software system at uniform time intervals may not be optimal. We also find that an increase in software reuse leads to more frequent replacement, but the number of maintenance activities is not significantly impacted.",Index Terms- Software maintenance and replacement;cost models;optimal scheduling.
28,The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment,IEEE Transactions on Software Engineering,2011,695,707,10.1109/TSE.2010.78,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551161,"Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects.",Cost estimation;software psychology;requirements/specifications.
29,In Memoriam - David Notkin (1953-2013),IEEE Transactions on Software Engineering,2013,742,743,10.1109/TSE.2013.25,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519247,"David Samuel Notkin, whose technical, educational, and social contributions to computer science and software engineering research made him a major figure in the field, died on 22 April 2013, at his home in Seattle, Washington. He was 58 years old. The cause of his death was cancer. David is best known for his research, with his many graduate students, on software evolution. He asked why software is often so hard and expensive to change, and he worked to reduce the difficulty of software evolution to an essential minimum. This focus came from his belief that the ability to change software - its softness - is where its true but under-realized potential resides. He asked questions such as whether we can identify and close the gap between Brooks' notions of accidental and essential software complexity? How much should rather than does it cost to develop, test, and evolve software? Can we make the cost of change proportionate rather than disproportionate to the apparent complexity of changes to be made? Can we design software analysis methods that realize the best properties of both static and dynamic analysis techniques? Beyond technical contributions, David is widely recognized and admired for his exceptional skill as a research mentor for graduate students and as a powerful and unwavering advocate for improving gender diversity in computer science. A brief biography is given highlighting Notkin's professional achievements.",
0,Model-Driven Fault Injection in Java Source Code,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),2020,414,425,10.1109/ISSRE5003.2020.00046,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251070,"The injection of software faults in source code requires accurate knowledge of the programming language, both to craft faults and to identify injection locations. As such, fault injection and code mutation tools are typically tailored for a specific language and have limited extensibility. In this paper we present a model-driven approach to craft and inject software faults in source code. While its concrete application is presented for Java, the workflow we propose does not depend on a specific programming language. Following Model-Driven Engineering principles, the faults and the criteria to select injection locations are described using structured, machine-readable specifications based on a domain-specific language. Then, automated transformations craft artifacts based on OCL and Java, which represent the faults to be injected and are able to select the candidate injection locations. Finally, artifacts are executed against the target source code, performing the injection in the desired locations. We devise a supporting tool and exercise the approach injecting 13 different kinds of software faults in the Java source code of six different projects.",Software faults;fault libraries;metamodel;OCL;code patterns;Java.
1,QoS-aware Metamorphic Testing: An Elevation Case Study,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),2020,104,114,10.1109/ISSRE5003.2020.00019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251056,"Elevators are among the oldest and most widespread transportation systems, yet their complexity increases rapidly to satisfy customization demands and to meet quality of service requirements. Verification and validation tasks in this context are costly, since they rely on the manual intervention of domain experts at some points of the process. This is mainly due to the difficulty to assess whether the elevators behave as expected in the different test scenarios, the so-called test oracle problem. Metamorphic testing is a thriving testing technique that alleviates the oracle problem by reasoning on the relations among multiple executions of the system under test, the so-called metamorphic relations. In this practical experience paper, we report on the application of metamorphic testing to verify an industrial elevator dispatcher. Together with domain experts from the elevation sector, we defined multiple metamorphic relations that consider domain-specific quality of service measures. Evaluation results with seeded faults show that the approach is effective at detecting faults automatically.",Cyber-Physical Systems;Elevators;Metamorphic Testing;Quality of Service
0,Self-Organizing Roles on Agile Software Development Teams,IEEE Transactions on Software Engineering,2013,422,444,10.1109/TSE.2012.30,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197202,"Self-organizing teams have been recognized and studied in various forms-as autonomous groups in socio-technical systems, enablers of organizational theories, agents of knowledge management, and as examples of complex-adaptive systems. Over the last decade, self-organizing teams have taken center stage in software engineering when they were incorporated as a hallmark of Agile methods. Despite the long and rich history of self-organizing teams and their recent popularity with Agile methods, there has been little research on the topic within software wngineering. Particularly, there is a dearth of research on how Agile teams organize themselves in practice. Through a Grounded Theory research involving 58 Agile practitioners from 23 software organizations in New Zealand and India over a period of four years, we identified informal, implicit, transient, and spontaneous roles that make Agile teams self-organizing. These roles-Mentor, Coordinator, Translator, Champion, Promoter, and Terminator-are focused toward providing initial guidance and encouraging continued adherence to Agile methods, effectively managing customer expectations and coordinating customer collaboration, securing and sustaining senior management support, and identifying and removing team members threatening the self-organizing ability of the team. Understanding these roles will help software development teams and their managers better comprehend and execute their roles and responsibilities as a self-organizing team.",Self-organizing;team roles;software engineering;Agile software development;grounded theory
13,Coordination Challenges in Large-Scale Software Development: A Case Study of Planning Misalignment in Hybrid Settings,IEEE Transactions on Software Engineering,2018,932,950,10.1109/TSE.2017.2730870,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990187,"Achieving effective inter-team coordination is one of the most pressing challenges in large-scale software development. Hybrid approaches of traditional and agile development promise combining the overview and predictability of long-term planning on an inter-team level with the flexibility and adaptability of agile development on a team level. It is currently unclear, however, why such hybrids often fail. Our case study within a large software development unit of 13 teams at a global enterprise software company explores how and why a combination of traditional planning on an inter-team level and agile development on a team level can result in ineffective coordination. Based on a variety of data, including interviews with scrum masters, product owners, architects and senior management, and using Grounded Theory data analysis procedures, we identify a lack of dependency awareness across development teams as a key explanation of ineffective coordination. Our findings show how a lack of dependency awareness emerges from misaligned planning activities of specification, prioritization, estimation and allocation between agile team and traditional inter-team levels and ultimately prevents effective coordination. Knowing about these issues, large-scale hybrid projects in similar contexts can try to better align their planning activities across levels to improve dependency awareness and in turn achieve more effective coordination.",Large-scale software development;agile;hybrid;inter-team coordination;dependency awareness;planning alignment;information systems development
15,Comparing the Defect Reduction Benefits of Code Inspection and Test-Driven Development,IEEE Transactions on Software Engineering,2012,547,560,10.1109/TSE.2011.46,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750007,"This study is a quasi experiment comparing the software defect rates and implementation costs of two methods of software defect reduction: code inspection and test-driven development. We divided participants, consisting of junior and senior computer science students at a large Southwestern university, into four groups using a two-by-two, between-subjects, factorial design and asked them to complete the same programming assignment using either test-driven development, code inspection, both, or neither. We compared resulting defect counts and implementation costs across groups. We found that code inspection is more effective than test-driven development at reducing defects, but that code inspection is also more expensive. We also found that test-driven development was no more effective at reducing defects than traditional programming methods.",Agile programming;code inspections and walk throughs;reliability;test-driven development;testing strategies;empirical study.
16,Evaluating the effect of a delegated versus centralized control style on the maintainability of object-oriented software,IEEE Transactions on Software Engineering,2004,521,534,10.1109/TSE.2004.43,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1316869,"A fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a ""bad"" object-oriented design. We present a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate, and senior professional consultants from several international consultancy companies were hired for one day to participate in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs that had a centralized and delegated control style, respectively. The results show that the most skilled developers, in particular, the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular, the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. Thus, the maintainability of object-oriented software depends, to a large extent, on the skill of the developers who are going to maintain it. These results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems may eventually pose difficulties unless they make an effort to keep the designs simple, as the cognitive complexity of ""expert"" designs might be unmanageable for less skilled maintainers.",Index Terms- Design principles;responsibility delegation;control styles;object-oriented design;object-oriented programming;software maintainability;controlled experiment.
17,Evaluating Pair Programming with Respect to System Complexity and Programmer Expertise,IEEE Transactions on Software Engineering,2007,65,86,10.1109/TSE.2007.17,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052584,"A total of 295 junior, intermediate, and senior professional Java consultants (99 individuals and 98 pairs) from 29 international consultancy companies in Norway, Sweden, and the UK were hired for one day to participate in a controlled experiment on pair programming. The subjects used professional Java tools to perform several change tasks on two alternative Java systems with different degrees of complexity. The results of this experiment do not support the hypotheses that pair programming in general reduces the time required to solve the tasks correctly or increases the proportion of correct solutions. On the other hand, there is a significant 84 percent increase in effort to perform the tasks correctly. However, on the more complex system, the pair programmers had a 48 percent increase in the proportion of correct solutions but no significant differences in the time taken to solve the tasks correctly. For the simpler system, there was a 20 percent decrease in time taken but no significant differences in correctness. However, the moderating effect of system complexity depends on the programmer expertise of the subjects. The observed benefits of pair programming in terms of correctness on the complex system apply mainly to juniors, whereas the reductions in duration to perform the tasks correctly on the simple system apply mainly to intermediates and seniors. It is possible that the benefits of pair programming will exceed the results obtained in this experiment for larger, more complex tasks and if the pair programmers have a chance to work together over a longer period of time",Empirical software engineering;pair programming;extreme programming;design principles;control styles;object-oriented programming;software maintainability;quasi-experiment.
19,Measuring Program Comprehension: A Large-Scale Field Study with Professionals,IEEE Transactions on Software Engineering,2018,951,976,10.1109/TSE.2017.2734091,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997917,"During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend ~58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc.",Program comprehension;field study;inference model
20,Investigating the Impact of Development Task on External Quality in Test-Driven Development: An Industry Experiment,IEEE Transactions on Software Engineering,2019,1,1,10.1109/TSE.2019.2949811,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8884172,"Reviews on test-driven development (TDD) studies suggest that the conflicting results reported in the literature are due to unobserved factors, such as the tasks used in the experiments, and highlight that there are very few industry experiments conducted with professionals. The goal of this study is to investigate the impact of a new factor, the chosen task, and the development approach on external quality in an industrial experiment setting with 17 professionals. The participants are junior to senior developers in programming with Java, beginner to novice in unit testing, JUnit, and they have no prior experience in TDD. The experimental design is a <formula><tex>$2 \times 2$</tex></formula> cross-over, i.e., we use two tasks for each of the two approaches, namely TDD and incremental test-last development (ITLD). Our results reveal that both development approach and task are significant factors with regards to the external quality achieved by the participants. More specifically, the participants produce higher quality code during ITLD in which splitting user stories into subtasks, coding, and testing activities are followed, compared to TDD. The results also indicate that the participants produce higher quality code during the implementation of Bowling Score Keeper, compared to that of Mars Rover API, although they perceived both tasks as of similar complexity. An interaction between the development approach and task could not be observed in this experiment. We conclude that variables that have not been explored so often, such as the extent to which the task is specified in terms of smaller subtasks, and developers' unit testing experience might be critical factors in TDD experiments. The real-world appliance of TDD and its implications on external quality still remain to be challenging unless these uncontrolled and unconsidered factors are further investigated by researchers in both academic and industrial settings.",Test-driven development;industry experiment;experimental task;incremental test-last development;external quality
21,Effects of Developer Experience on Learning and Applying Unit Test-Driven Development,IEEE Transactions on Software Engineering,2014,381,395,10.1109/TSE.2013.2295827,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690135,"Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.",Test-driven development;test-first design;software engineering process;software quality/SQA;software construction;process conformance;programmer productivity
22,Moving from Closed to Open Source: Observations from Six Transitioned Projects to GitHub,IEEE Transactions on Software Engineering,2021,1838,1856,10.1109/TSE.2019.2937025,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812899,"Open source software systems have gained a lot of attention in the past few years. With the emergence of open source platforms like GitHub, developers can contribute, store, and manage their projects with ease. Large organizations like Microsoft, Google, and Facebook are open sourcing their in-house technologies in an effort to more broadly involve the community in the development of software systems. Although closed source and open source systems have been studied extensively, there has been little research on the transition from closed source to open source systems. Through this study we aim to: a) provide guidance and insights for other teams planning to open source their projects and b) to help them avoid pitfalls during the transition process. We studied six different Microsoft systems, which were recently open-sourced i.e., CoreFX, CoreCLR, Roslyn, Entity Framework, MVC, and Orleans. This paper presents the transition from the viewpoints of both Microsoft and the open source community based on interviews with eleven Microsoft developer, five Microsoft senior managers involved in the decision to open source, and eleven open-source developers. From Microsoft鈥檚 perspective we discuss the reasons for the transition, experiences of developers involved, and the transition鈥檚 outcomes and challenges. Our results show that building a vibrant community, prompt answers, developing an open source culture, security regulations and business opportunities are the factors which persuade companies to open source their products. We also discuss the transition outcomes on processes such as code reviews, version control systems, continuous integration as well as developers鈥 perception of these changes. From the open source community鈥檚 perspective, we illustrate the response to the open-sourcing initiative through contributions and interactions with the internal developers and provide guidelines for other projects planning to go open source.",Empirical study;GitHub;open-source;Microsoft
23,Corrections to "the effectiveness of control structure diagrams in source code comprehension activities",IEEE Transactions on Software Engineering,2002,624,624,10.1109/TSE.2002.1010064,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010064,,
